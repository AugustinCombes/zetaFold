{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRzrLyDQBwLF",
        "outputId": "adfb4d70-7d28-453e-dae7-fe4a4397b290"
      },
      "outputs": [],
      "source": [
        "# ## COLAB AREA\n",
        "\n",
        "# !git clone https://github.com/AugustinCombes/zetaFold.git\n",
        "# %pip install transformers\n",
        "# # from google.colab import drive\n",
        "\n",
        "# # drive.mount('/content/gdrive/', force_remount=True)\n",
        "# # ../gdrive/MyDrive/data\n",
        "# %cd zetaFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zjBShNTJBnOn"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange, tqdm\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from utils import normalize_adjacency, sparse_mx_to_torch_sparse_tensor, load_data, submit_predictions\n",
        "from dataloader import get_loader\n",
        "\n",
        "from sklearn.metrics import log_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5hufD8Q2BnOq"
      },
      "outputs": [],
      "source": [
        "n_labels = 4888\n",
        "treshold_valid = 733\n",
        "\n",
        "ref = dict()\n",
        "with open('data/graph_labels.txt', 'r') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        t = line.split(',')\n",
        "        if len(t[1][:-1]) != 0:\n",
        "            if len([_ for _ in ref.values() if _ == \"train\"]) < n_labels - treshold_valid:\n",
        "              ref[i] = \"train\"\n",
        "            else :\n",
        "              ref[i] = \"valid\"\n",
        "        else:\n",
        "            ref[i] = \"test\"\n",
        "\n",
        "ref_train = np.array([i for i in range(len(ref)) if ref[i]==\"train\"])\n",
        "ref_valid = np.array([i for i in range(len(ref)) if ref[i]==\"valid\"])\n",
        "ref_test = np.array([i for i in range(len(ref)) if ref[i]==\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_H9LliJSBnOr"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "# device = 'mps'\n",
        "# device = 'cuda'\n",
        "\n",
        "version = \"train\"\n",
        "# version = \"valid\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nNXFYx-q3Teo"
      },
      "outputs": [],
      "source": [
        "y = []\n",
        "valid_id = []\n",
        "\n",
        "with open('data/graph_labels.txt', \"r\") as f1:\n",
        "  for line in f1:\n",
        "    s1, s2 = line.strip().split(',')\n",
        "    if len(s2.strip())>0:\n",
        "      y.append(int(s2))\n",
        "    else :\n",
        "      valid_id.append(s1)\n",
        "\n",
        "y = np.array(y)\n",
        "y_train, y_valid = y[:-treshold_valid], y[-treshold_valid:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfo59lxfBnOt"
      },
      "source": [
        "# SEQUENCES ONLY : Finetune DistillProtbert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s2FWjH493mN9"
      },
      "outputs": [],
      "source": [
        "sequences = [] \n",
        "with open('data/sequences.txt', \"r\") as f1:\n",
        "  for line in f1:\n",
        "    sequences.append(' '.join(list(line[:-1])))\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "sequences_train, sequences_valid, sequences_test = sequences[ref_train], sequences[ref_valid], sequences[ref_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "12bfd59b85bf4b068665020cfd0a934d",
            "d607a63ac1f9465ca48d85696f0c6ec8",
            "81d7dbdc10ab491485baa5ee11568705",
            "d0d5713810b24e06a971f51a5f1d957f",
            "3c90639f8f3a4c84856ae48893e542dd",
            "257f5aa5336b4c2487462de36fbd5921",
            "a1e24dad435d4f6b8092d3ffbf456f9b",
            "aea5bd8f08a54a9190e1b7160312353a",
            "b44fd31d6b51446398370f565020139d",
            "3200dd4f52484afd9d397218eeaccd26",
            "d9c6fb18a7754d5f96aff226ff39255d",
            "d661bddd1e8b429e9c4672200e7fedd2",
            "f0123564ce2c4dd9a213ab35589ab683",
            "28b01989f7e04de5948f2b78706650ae",
            "5c49923877ec457ca0dcbc37d17dfb76",
            "9e17c823ad624339954e5fd87641059f",
            "f516161f956d46fbb8230c0298781bdc",
            "25ed55a13cda451396c20c0624440dcc",
            "3e6c121bc7c549158dc9c048b60b7234",
            "f88b7d13d9b640309aa4662a9a28993e",
            "ae37cbd255844fecb73f0e20671501ee",
            "e9bee3614d0b44888ed357583a4cdcca",
            "a8f1983b26da497da540f53104c0b26f",
            "a28e039920664a7abbd108eda586e0ba",
            "53552a1add4844b1aa254cf97f359aef",
            "f593ab09239f43b9b09492dbe4785927",
            "819e2fb14dfb46beac0bad721e8aa6f0",
            "f6b2e6b1b57c4456a075a9705a8e4ae6",
            "f8a725f7e71d47ccb599c6338f3bc5c5",
            "d1e2069e252e44178624103d4bac9037",
            "341b52c1046e4ca8a45024348e3f3c1c",
            "19488068a7ec4d94967be6e3f04b95d4",
            "17c8bbaf3b9e49708dd87a882f697f5d",
            "f76bc551a8bf4e9a8dd081c1aa4ddf0d",
            "7ed0a918143242dda018f84ed93caa4e",
            "45458a80fa5a4f058cd05dab4061ad6d",
            "573bd8fd266c40d387b07d064f3bde72",
            "49423cc17b01474e819b9a03e6e9a9bb",
            "dfecb7c6212d44539b79d42e4f887b5f",
            "83288d42e5534a8aacb998b7d6d5f651",
            "bd6933c3e25949f18b9930c09133ac8d",
            "488be0d55b2d46f1af04cf01c9a6cda7",
            "b8ffa6e98a37409eb66eee1879ba180e",
            "8d497159d5c644ab93e914bc94eda822"
          ]
        },
        "id": "X7GTZu6DBnOu",
        "outputId": "1a993476-cdeb-4156-db65-4fc91ae8435f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12bfd59b85bf4b068665020cfd0a934d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d661bddd1e8b429e9c4672200e7fedd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8f1983b26da497da540f53104c0b26f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f76bc551a8bf4e9a8dd081c1aa4ddf0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/589 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'yarongef/DistilProtBert'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ynVmvgPIBnOu"
      },
      "outputs": [],
      "source": [
        "## Define probert-based classifier\n",
        " \n",
        "class ProteinClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(ProteinClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME).to(device)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
        "        output = self.bert(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask\n",
        "        )\n",
        "        output = output.last_hidden_state[:, 0, :]\n",
        "        output = nn.Dropout(0.2)(output) # 0.3 meilleure submission\n",
        "        output = nn.ReLU()(output)\n",
        "        output = self.classifier(output)\n",
        "        # output = nn.ReLU()(output)\n",
        "        # return output\n",
        "\n",
        "        return nn.LogSoftmax(dim=1)(output)\n",
        "\n",
        "# model = ProteinClassifier(18).to(device)\n",
        "\n",
        "# for module in model.bert.encoder.layer[0:-1]:\n",
        "#     for param in module.parameters():\n",
        "#         param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u72E7pCrBnOw"
      },
      "source": [
        "# GRAPHE : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lm1PxVJVBnOx"
      },
      "outputs": [],
      "source": [
        "## Prepare graph features\n",
        "\n",
        "# 1. Full graph-related database in arrays\n",
        "\n",
        "adj, adj_weight, features, edge_features, Flist = load_data()\n",
        "del Flist\n",
        "del edge_features\n",
        "\n",
        "adj = [normalize_adjacency(A, W) for A, W in zip(adj, adj_weight)]\n",
        "adj_shapes = np.array([at.shape[0] for at in adj])\n",
        "adj = np.array([adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))])\n",
        "\n",
        "features = np.array(features, dtype=object)\n",
        "\n",
        "adj_train, adj_valid, adj_test = adj[ref_train], adj[ref_valid], adj[ref_test]\n",
        "features_train, features_valid, features_test = features[ref_train], features[ref_valid], features[ref_test]\n",
        "adj_shapes_train, adj_shapes_valid, adj_shapes_test = adj_shapes[ref_train], adj_shapes[ref_valid], adj_shapes[ref_test]\n",
        "\n",
        "\n",
        "# features = features[:n_labels] if version != \"valid\" else features[n_labels:]\n",
        "# adj = adj[:n_labels] if version != \"valid\" else adj[n_labels:]\n",
        "\n",
        "## 2. Then, we load batchs thanks to \"indexs\" key of dataloader element\n",
        "\n",
        "## Example : loading only the first batch of graph-related data\n",
        "# for e in data_loader:\n",
        "#     break\n",
        "# batch_indices = e['indexs'] ## Indexs that are part of the batch\n",
        "\n",
        "# features_ = np.array(features)[batch_indices]\n",
        "# features_ = np.vstack(features_)\n",
        "# features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "# adj_ = adj[batch_indices]\n",
        "# adj_ = sp.block_diag(adj_)\n",
        "# adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "# cum_nodes = adj_shapes[batch_indices]\n",
        "# idx_batch = np.repeat(np.arange(\n",
        "#     len(batch_indices)\n",
        "#     ), cum_nodes)\n",
        "# idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "\n",
        "# # With GNN model\n",
        "\n",
        "# model = GNN(...).to(device)\n",
        "# output = model(features_, adj_, idx_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Prepare graph features\n",
        "\n",
        "# 1. Full graph-related database in arrays\n",
        "\n",
        "adj, adj_weight, features, edge_features, Flist = load_data()\n",
        "del Flist\n",
        "del edge_features\n",
        "\n",
        "adj = [normalize_adjacency(A, W) for A, W in zip(adj, adj_weight)]\n",
        "adj_shapes = np.array([at.shape[0] for at in adj])\n",
        "adj = np.array([adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))])\n",
        "\n",
        "features = np.array(features, dtype=object)\n",
        "\n",
        "adj_train, adj_valid, adj_test = adj[ref_train], adj[ref_valid], adj[ref_test]\n",
        "features_train, features_valid, features_test = features[ref_train], features[ref_valid], features[ref_test]\n",
        "adj_shapes_train, adj_shapes_valid, adj_shapes_test = adj_shapes[ref_train], adj_shapes[ref_valid], adj_shapes[ref_test]\n",
        "\n",
        "\n",
        "# features = features[:n_labels] if version != \"valid\" else features[n_labels:]\n",
        "# adj = adj[:n_labels] if version != \"valid\" else adj[n_labels:]\n",
        "\n",
        "## 2. Then, we load batchs thanks to \"indexs\" key of dataloader element\n",
        "\n",
        "## Example : loading only the first batch of graph-related data\n",
        "# for e in data_loader:\n",
        "#     break\n",
        "# batch_indices = e['indexs'] ## Indexs that are part of the batch\n",
        "\n",
        "shuffle_train = np.arange(len(sequences_train))\n",
        "indices = shuffle_train[:32]\n",
        "\n",
        "features_ = np.array(features_train)[indices]\n",
        "features_ = np.vstack(features_)\n",
        "features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "adj_ = adj_train[indices]\n",
        "adj_ = sp.block_diag(adj_)\n",
        "adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "cum_nodes = adj_shapes_train[indices]\n",
        "idx_batch = np.repeat(np.arange(\n",
        "    len(indices)\n",
        "    ), cum_nodes)\n",
        "idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "\n",
        "# # With GNN model\n",
        "\n",
        "# model = GNN(...).to(device)\n",
        "# output = model(features_, adj_, idx_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8221, 86])"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# OLD\n",
        "\n",
        "# features_ = np.array(features_train)[indices]\n",
        "# features_ = np.vstack(features_)\n",
        "# features_ = torch.FloatTensor(features_).to(device)\n",
        "# features_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8221, 86])\n",
            "74646\n"
          ]
        }
      ],
      "source": [
        "# NEW\n",
        "\n",
        "# layer = nn.TransformerEncoderLayer(d_model=86, nhead=2, dim_feedforward=256, batch_first=True)\n",
        "\n",
        "# features_ = np.array(features_train)[indices]\n",
        "# features_ = torch.nn.utils.rnn.pad_sequence(\n",
        "#     [torch.tensor(f, dtype=torch.float32) for f in features_],\n",
        "#     batch_first=True\n",
        "#     )\n",
        "# mask = torch.nn.utils.rnn.pad_sequence([torch.zeros(e) for e in cum_nodes], batch_first=True, padding_value=1).type(torch.bool)#\n",
        "# features_ = layer(features_, src_key_padding_mask=mask)\n",
        "# features_ = torch.vstack([features_[j,:c] for j,c in enumerate(cum_nodes)])\n",
        "# print(features_.shape)\n",
        "\n",
        "# for k,v in layer.state_dict().items():\n",
        "#     # print(k, v.shape)\n",
        "#     ()\n",
        "# 258*86+258+86*86+86+256*86+256+86*256+86*5    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GpIToMxiBnOx"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple message passing model that consists of 2 message passing layers\n",
        "    and the sum aggregation function\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super(GNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # self.fc2 = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8)\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, n_class)\n",
        "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x_in, adj, idx):\n",
        "        # first message passing layer\n",
        "        x = self.fc1(x_in)\n",
        "        x = self.relu(torch.mm(adj, x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # second message passing layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(torch.mm(adj, x))\n",
        "        \n",
        "        # sum aggregator\n",
        "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
        "        out = torch.zeros(torch.max(idx)+1, x.size(1)).to(x_in.device)\n",
        "        out = out.scatter_add_(0, idx, x)\n",
        "        \n",
        "        # batch normalization layer\n",
        "        out = self.bn(out)\n",
        "\n",
        "        # mlp to produce output\n",
        "        out = self.relu(self.fc3(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc4(out)\n",
        "        \n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "model = GNN(86, 64, 0.2, 18).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple message passing model that consists of 2 message passing layers\n",
        "    and the sum aggregation function\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super(GNN, self).__init__()\n",
        "        # self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc1 = nn.TransformerEncoderLayer(d_model=86, nhead=2, dim_feedforward=256, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, n_class)\n",
        "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x_in, adj, idx, cum_nodes):\n",
        "        # first message passing layer\n",
        "        pad_mask = torch.nn.utils.rnn.pad_sequence([torch.zeros(e) for e in cum_nodes], batch_first=True, padding_value=1).type(torch.bool)\n",
        "        x = self.fc1(x_in, src_key_padding_mask=pad_mask)\n",
        "        print('1', x.shape)\n",
        "        x = torch.vstack([x[j,:c] for j,c in enumerate(cum_nodes)])\n",
        "        print('2', x.shape)\n",
        "        x = self.relu(torch.mm(adj, x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # second message passing layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(torch.mm(adj, x))\n",
        "        \n",
        "        # sum aggregator\n",
        "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
        "        out = torch.zeros(torch.max(idx)+1, x.size(1)).to(x_in.device)\n",
        "        out = out.scatter_add_(0, idx, x)\n",
        "        \n",
        "        # batch normalization layer\n",
        "        out = self.bn(out)\n",
        "\n",
        "        # mlp to produce output\n",
        "        out = self.relu(self.fc3(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc4(out)\n",
        "        \n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "model = GNN(86, 64, 0.2, 18).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 torch.Size([64, 796, 86])\n",
            "2 torch.Size([14822, 86])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (14822x86 and 64x64)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[133], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m adj_ \u001b[39m=\u001b[39m sparse_mx_to_torch_sparse_tensor(adj_)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     62\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 63\u001b[0m output \u001b[39m=\u001b[39m model(features_, adj_, idx_batch, cum_nodes)\n\u001b[1;32m     65\u001b[0m \u001b[39mdel\u001b[39;00m features_\n\u001b[1;32m     66\u001b[0m \u001b[39mdel\u001b[39;00m adj_\n",
            "File \u001b[0;32m~/Desktop/envs/altegrad_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[132], line 29\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x_in, adj, idx, cum_nodes)\u001b[0m\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     28\u001b[0m \u001b[39m# second message passing layer\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x)\n\u001b[1;32m     30\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(torch\u001b[39m.\u001b[39mmm(adj, x))\n\u001b[1;32m     32\u001b[0m \u001b[39m# sum aggregator\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/envs/altegrad_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Desktop/envs/altegrad_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (14822x86 and 64x64)"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), \n",
        "    # lr=1,\n",
        "    lr=1e-3,\n",
        "    # weight_decay=0.0001\n",
        "    )\n",
        "epochs = 50\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
        "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-4) * ((epoch - epochs)/ epochs))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "shuffle_train = np.arange(len(sequences_train))\n",
        "shuffle_valid = np.arange(len(sequences_valid))\n",
        "\n",
        "batchSize = 64\n",
        "\n",
        "patience = 50\n",
        "patience_count = 0\n",
        "\n",
        "previous_epoch_loss_valid = 300\n",
        "\n",
        "res = list()\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    epoch_loss, epoch_loss_valid, epoch_accuracy, epoch_accuracy_valid = [], [], [], []\n",
        "\n",
        "    np.random.shuffle(shuffle_train)\n",
        "    np.random.shuffle(shuffle_valid)\n",
        "\n",
        "\n",
        "    # Train\n",
        "    # batchSize=2\n",
        "    model.train()\n",
        "    for i in range(0, len(sequences_train), batchSize):\n",
        "\n",
        "        incr_i = min(i+batchSize, len(sequences_train))\n",
        "        indices = shuffle_train[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_train[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "        \n",
        "        # features_ = np.array(features_train)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "        features_ = np.array(features_train)[indices]\n",
        "        features_ = torch.nn.utils.rnn.pad_sequence(\n",
        "            [torch.tensor(f, dtype=torch.float32) for f in features_],\n",
        "            batch_first=True\n",
        "            ).to(device)\n",
        "\n",
        "        cum_nodes = adj_shapes_train[indices]\n",
        "        idx_batch = np.repeat(np.arange(\n",
        "            batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
        "            ), cum_nodes)\n",
        "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        adj_ = adj_train[indices]\n",
        "        adj_ = sp.block_diag(adj_)\n",
        "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features_, adj_, idx_batch, cum_nodes)\n",
        "        \n",
        "        del features_\n",
        "        del adj_\n",
        "        del idx_batch\n",
        "\n",
        "        # Backward\n",
        "        loss = criterion(output, targets_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute metrics\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # Validation\n",
        "    # batchSize=64\n",
        "    model.eval()\n",
        "    for i in range(0, len(sequences_valid), int(batchSize)):\n",
        "\n",
        "        incr_i = min(i+int(batchSize), len(sequences_valid))\n",
        "        indices = shuffle_valid[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_valid[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "\n",
        "        # features_ = np.array(features_train)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "        features_ = np.array(features_train)[indices]\n",
        "        features_ = torch.nn.utils.rnn.pad_sequence(\n",
        "            [torch.tensor(f, dtype=torch.float32) for f in features_],\n",
        "            batch_first=True\n",
        "            ).to(device)\n",
        "\n",
        "        cum_nodes = adj_shapes_valid[indices]\n",
        "        idx_batch = np.repeat(np.arange(\n",
        "            int(batchSize) if incr_i%int(batchSize)==0 else incr_i%int(batchSize)\n",
        "            ), cum_nodes)\n",
        "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        adj_ = adj_valid[indices]\n",
        "        adj_ = sp.block_diag(adj_)\n",
        "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        output = model(features_, adj_, idx_batch, cum_nodes)\n",
        "\n",
        "        del features_\n",
        "        del adj_\n",
        "        del idx_batch\n",
        "\n",
        "        # Compute metrics\n",
        "        loss = criterion(output, targets_)\n",
        "        epoch_loss_valid.append(loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy_valid.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "    epoch_loss = np.array(epoch_loss).mean()\n",
        "    epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
        "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "    epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
        "\n",
        "    tqdm.write(\n",
        "        '\\n'\n",
        "        f'Epoch {epoch}:\\ntrain loss: {round(epoch_loss, 4)}, '\n",
        "        f'valid loss: {round(epoch_loss_valid, 4)}, delta loss: {round(epoch_loss-epoch_loss_valid, 4)},\\nacc train: {round(epoch_accuracy, 4)}, '\n",
        "        f'acc valid: {round(epoch_accuracy_valid, 4)}'\n",
        "        )\n",
        "    \n",
        "    # Early stopping :\n",
        "    if previous_epoch_loss_valid < epoch_loss_valid:\n",
        "        if patience_count == 0:\n",
        "          torch.save({\"state\": model.state_dict(),}, \"last_model_checkpoint.pt\")\n",
        "        patience_count +=1\n",
        "        if patience_count == patience:\n",
        "          print(f'Early stopping end of epoch {epoch}')\n",
        "          break\n",
        "\n",
        "    else :\n",
        "        previous_epoch_loss_valid = epoch_loss_valid\n",
        "        patience_count = 0\n",
        "\n",
        "        # Save last best results\n",
        "        del res\n",
        "        res = list()\n",
        "        shuffle_test = np.arange(len(sequences_test))\n",
        "\n",
        "        for i in range(0, len(sequences_test), int(batchSize)):\n",
        "            model.eval()\n",
        "\n",
        "            incr_i = min(i+int(batchSize), len(sequences_test))\n",
        "            indices = shuffle_test[i: incr_i]\n",
        "\n",
        "            features_ = np.array(features_test)[indices]\n",
        "            features_ = np.vstack(features_)\n",
        "            features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "            cum_nodes = adj_shapes_test[indices]\n",
        "            idx_batch = np.repeat(np.arange(\n",
        "                batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
        "                ), cum_nodes)\n",
        "            idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "            \n",
        "            adj_ = adj_test[indices]\n",
        "            adj_ = sp.block_diag(adj_)\n",
        "            adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "            output = model(features_, adj_, idx_batch)\n",
        "\n",
        "            res.append(output.to('cpu').detach().numpy())\n",
        "\n",
        "            del output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "k = 0\n",
        "\n",
        "résumé = []\n",
        "for k in range(18):\n",
        "    bol = tf.argmax(y_test, axis=1) == k\n",
        "    preds = model(X_test)[bol]\n",
        "    targets = y_test[bol]\n",
        "    accuracy_preds = (tf.argmax(preds, axis=1)==k).numpy().mean()\n",
        "    nll = log_loss(y_true=targets, y_pred=preds)\n",
        "    résumé.append((accuracy_preds, nll))\n",
        "\n",
        "acc = [tup[0] for tup in résumé]\n",
        "nll = [tup[1] for tup in résumé]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, tight_layout=True, figsize=(14, 6))\n",
        "axs[0].bar(np.arange(len(acc)), acc)\n",
        "axs[0].set_title('Accuracy')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "axs[1].bar(np.arange(len(nll)), nll)\n",
        "axs[1].set_title('Negative Log Likelihood')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "axs[2].bar(np.arange(len(nll)), [440.,  50., 939.,  60., 112., 625., 202.,  74., 998.,  57.,  43.,305.,  44.,  59., 548., 226.,  60.,  46.])\n",
        "axs[2].set_title('Pops')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "1F1E96OP9ICA",
        "outputId": "22841bb5-d726-4d39-9154-6093f59acfee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4888 4888 4888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8, cel: 2.3809, nll: 2.3809, acc: 0.2111:  18%|█▊        | 9/50 [00:46<03:32,  5.19s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-898958f36e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0madj_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0madj_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_diag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0madj_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_mx_to_torch_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mblock_diag\u001b[0;34m(mats, format, dtype)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtocoo\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         return coo_matrix((self.data, (row, col)), self.shape, copy=copy,\n\u001b[0m\u001b[1;32m   1032\u001b[0m                           dtype=self.dtype)\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row index exceeds matrix dimensions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     38\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     39\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = GNN(86, 64, 0.1, 18).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "epochs = 50\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
        "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-3) * ((epoch - epochs)/ epochs))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "data_loader = get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
        "                tokenizer=tokenizer, max_len=600, batch_size=64, shuffle=False, version=version, drop_last=True)\n",
        "                \n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    epoch_loss, epoch_log_loss, epoch_accuracy = [], [], []\n",
        "\n",
        "    for batch_num, e in enumerate(data_loader):\n",
        "        \n",
        "        batch_indices = e['indexs']\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target = F.one_hot(e['target'], 18).float().to(device)\n",
        "\n",
        "        # Compute graph forward\n",
        "        features_ = np.array(features)[batch_indices]\n",
        "        features_ = np.vstack(features_)\n",
        "        features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "        adj_ = adj[batch_indices]\n",
        "        adj_ = sp.block_diag(adj_)\n",
        "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        cum_nodes = adj_shapes[batch_indices]\n",
        "        idx_batch = np.repeat(np.arange(\n",
        "            len(batch_indices)\n",
        "            ), cum_nodes)\n",
        "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        output = model(features_, adj_, idx_batch)\n",
        "\n",
        "        # Backward\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute metrics\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        target = target.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "        nll_loss = log_loss(target, output)\n",
        "        epoch_log_loss.append(nll_loss)\n",
        "\n",
        "        accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy.append(accuracy)\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "    epoch_loss = np.array(epoch_loss).mean()\n",
        "    epoch_log_loss = np.array(epoch_log_loss).mean()\n",
        "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "\n",
        "    pbar.set_description(\n",
        "        f'Epoch {epoch}, cel: {round(epoch_loss, 4)}, '\n",
        "        f'nll: {round(epoch_log_loss, 4)}, acc: {round(epoch_accuracy, 4)}'\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhlE2OVR-DYF"
      },
      "outputs": [],
      "source": [
        "torch.save({\"state\": model.state_dict(),}, \"graphe_pretrain.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x34ufcZYBnOy"
      },
      "source": [
        "# MULTICULTURAL MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUyExhp-BnOy",
        "outputId": "6f17ceec-393d-4a56-c4aa-a5c721162452"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at yarongef/DistilProtBert were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at yarongef/DistilProtBert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# class ConcatModel(nn.Module):\n",
        "#     def __init__(self, out_dim, feature_dim, graph_dim):\n",
        "#         super(ConcatModel, self).__init__()\n",
        "#         # self.seqModel = ProteinClassifier(out_dim).to(device)\n",
        "#         # self.graphModel = GNN(feature_dim, graph_dim, dropout=0.25, n_class=out_dim).to(device)\n",
        "\n",
        "#         self.seqModel = ProteinClassifier(18).to(device)\n",
        "\n",
        "#         # self.classifier = nn.Linear(out_dim, 18)\n",
        "#         # self.activation = nn.LogSoftmax(dim=1)\n",
        "\n",
        "#     def forward(self, x_in, adj, idx, input_ids, attention_mask):\n",
        "#         seqEmbedding = self.seqModel(input_ids, attention_mask)\n",
        "#         # graphEmbedding = self.graphModel(x_in, adj, idx)\n",
        "        \n",
        "#         # out = torch.concat([graphEmbedding, seqEmbedding], dim=1)\n",
        "        \n",
        "#         # out = self.classifier(out)\n",
        "        \n",
        "#         # return self.activation(out)\n",
        "#         return nn.LogSoftmax(dim=1)(seqEmbedding)\n",
        "\n",
        "# model = ConcatModel(64, 86, 128).to(device)\n",
        "model = ProteinClassifier(18).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ylUrqjWXBnOy"
      },
      "outputs": [],
      "source": [
        "for module in model.bert.encoder.layer[:-1]:\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "for module in model.bert.encoder.layer[-1:]:\n",
        "    # module._modules[\"output\"].dense.weight.data.normal_(mean=0.0, std=0.1)\n",
        "    module._modules[\"output\"].dense.weight.data.uniform_(-0.1, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "VE3AFyv87MAh"
      },
      "outputs": [],
      "source": [
        "encode = lambda s : tokenizer.encode_plus(\n",
        "            s,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=600,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ob__pEhBnOz",
        "outputId": "b088dd90-1b66-43ff-d58f-ff34a9883abd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [03:52<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0:\n",
            "train loss: 1.9487, valid loss: 1.829, delta loss: 0.1196,\n",
            "acc train: 0.4297, acc valid: 0.4216\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 1/20 [08:15<1:23:19, 263.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1:\n",
            "train loss: 1.4058, valid loss: 1.4701, delta loss: -0.0643,\n",
            "acc train: 0.6049, acc valid: 0.5675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 2/20 [12:38<1:18:54, 263.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2:\n",
            "train loss: 1.1432, valid loss: 1.3205, delta loss: -0.1773,\n",
            "acc train: 0.6747, acc valid: 0.6153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 3/20 [17:01<1:14:30, 262.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3:\n",
            "train loss: 0.9378, valid loss: 1.2714, delta loss: -0.3336,\n",
            "acc train: 0.7435, acc valid: 0.6412\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 4/20 [21:23<1:10:07, 263.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4:\n",
            "train loss: 0.7524, valid loss: 1.2048, delta loss: -0.4524,\n",
            "acc train: 0.7952, acc valid: 0.6385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 5/20 [25:46<1:05:43, 262.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5:\n",
            "train loss: 0.5916, valid loss: 1.165, delta loss: -0.5734,\n",
            "acc train: 0.8446, acc valid: 0.6589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 6/20 [30:09<1:01:20, 262.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6:\n",
            "train loss: 0.4429, valid loss: 1.1887, delta loss: -0.7458,\n",
            "acc train: 0.8859, acc valid: 0.6712\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 7/20 [34:03<54:52, 253.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7:\n",
            "train loss: 0.3187, valid loss: 1.1319, delta loss: -0.8131,\n",
            "acc train: 0.9223, acc valid: 0.6849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 8/20 [38:26<51:16, 256.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8:\n",
            "train loss: 0.2109, valid loss: 1.2003, delta loss: -0.9895,\n",
            "acc train: 0.9569, acc valid: 0.6698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 9/20 [42:19<51:44, 282.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9:\n",
            "train loss: 0.1417, valid loss: 1.2104, delta loss: -1.0687,\n",
            "acc train: 0.9757, acc valid: 0.6835\n",
            "Early stopping end of epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), \n",
        "    # lr=1,\n",
        "    lr=3e-5,\n",
        "    # weight_decay=0.0001\n",
        "    )\n",
        "epochs = 20\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
        "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-4) * ((epoch - epochs)/ epochs))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "shuffle_train = np.arange(len(sequences_train))\n",
        "shuffle_valid = np.arange(len(sequences_valid))\n",
        "\n",
        "batchSize = 4\n",
        "\n",
        "patience = 2\n",
        "patience_count = 0\n",
        "\n",
        "previous_epoch_loss_valid = 300\n",
        "\n",
        "res = list()\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    epoch_loss, epoch_loss_valid, epoch_accuracy, epoch_accuracy_valid = [], [], [], []\n",
        "\n",
        "    np.random.shuffle(shuffle_train)\n",
        "    np.random.shuffle(shuffle_valid)\n",
        "\n",
        "\n",
        "    # Train\n",
        "    batchSize=2\n",
        "    model.train()\n",
        "    for i in range(0, len(sequences_train), batchSize):\n",
        "\n",
        "        incr_i = min(i+batchSize, len(sequences_train))\n",
        "        indices = shuffle_train[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_train[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "        \n",
        "        sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_train[indices]]).to(device)\n",
        "\n",
        "        attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_train[indices]]).to(device)\n",
        "\n",
        "        # features_ = np.array(features_train)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "        # cum_nodes = adj_shapes_train[indices]\n",
        "        # idx_batch = np.repeat(np.arange(\n",
        "        #     batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
        "        #     ), cum_nodes)\n",
        "        # idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        # adj_ = adj_train[indices]\n",
        "        # adj_ = sp.block_diag(adj_)\n",
        "        # adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(sequences_, attention_mask_)\n",
        "        \n",
        "        # del features_\n",
        "        # del adj_\n",
        "        # del idx_batch\n",
        "        del sequences_\n",
        "        del attention_mask_\n",
        "\n",
        "        # Backward\n",
        "        loss = criterion(output, targets_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute metrics\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # Validation\n",
        "    batchSize=4\n",
        "    model.eval()\n",
        "    for i in range(0, len(sequences_valid), int(batchSize/4)):\n",
        "\n",
        "        incr_i = min(i+int(batchSize/4), len(sequences_valid))\n",
        "        indices = shuffle_valid[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_valid[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "        \n",
        "        sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_valid[indices]]).to(device)\n",
        "\n",
        "        attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_valid[indices]]).to(device)\n",
        "\n",
        "        # features_ = np.array(features_valid)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "        # cum_nodes = adj_shapes_valid[indices]\n",
        "        # idx_batch = np.repeat(np.arange(\n",
        "        #     int(batchSize/4) if incr_i%int(batchSize/4)==0 else incr_i%int(batchSize/4)\n",
        "        #     ), cum_nodes)\n",
        "        # idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        # adj_ = adj_valid[indices]\n",
        "        # adj_ = sp.block_diag(adj_)\n",
        "        # adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        output = model(sequences_, attention_mask_)\n",
        "\n",
        "        # del features_\n",
        "        # del adj_\n",
        "        # del idx_batch\n",
        "        del sequences_\n",
        "        del attention_mask_\n",
        "\n",
        "        # Compute metrics\n",
        "        loss = criterion(output, targets_)\n",
        "        epoch_loss_valid.append(loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy_valid.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "    epoch_loss = np.array(epoch_loss).mean()\n",
        "    epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
        "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "    epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
        "\n",
        "    tqdm.write(\n",
        "        '\\n'\n",
        "        f'Epoch {epoch}:\\ntrain loss: {round(epoch_loss, 4)}, '\n",
        "        f'valid loss: {round(epoch_loss_valid, 4)}, delta loss: {round(epoch_loss-epoch_loss_valid, 4)},\\nacc train: {round(epoch_accuracy, 4)}, '\n",
        "        f'acc valid: {round(epoch_accuracy_valid, 4)}'\n",
        "        )\n",
        "    \n",
        "    # Early stopping :\n",
        "    if previous_epoch_loss_valid < epoch_loss_valid:\n",
        "        if patience_count == 0:\n",
        "          torch.save({\"state\": model.state_dict(),}, \"last_model_checkpoint.pt\")\n",
        "        patience_count +=1\n",
        "        if patience_count == patience:\n",
        "          print(f'Early stopping end of epoch {epoch}')\n",
        "          break\n",
        "\n",
        "    else :\n",
        "        previous_epoch_loss_valid = epoch_loss_valid\n",
        "        patience_count = 0\n",
        "\n",
        "        # Save last best results\n",
        "        del res\n",
        "        res = list()\n",
        "        shuffle_test = np.arange(len(sequences_test))\n",
        "\n",
        "        for i in range(0, len(sequences_test), int(batchSize/2)):\n",
        "            model.eval()\n",
        "\n",
        "            incr_i = min(i+int(batchSize/2), len(sequences_test))\n",
        "            sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_test[shuffle_test[i: incr_i]]])\n",
        "            attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_test[shuffle_test[i: incr_i]]])\n",
        "\n",
        "            output = model(sequences_, attention_mask_)\n",
        "\n",
        "            del sequences_\n",
        "            del attention_mask_\n",
        "            \n",
        "            res.append(output.to('cpu').detach().numpy())\n",
        "\n",
        "            del output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "IyP5geQYsLis",
        "outputId": "d58639f9-4dae-4756-fc21-c77f7385a2c3"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-476d3c0da290>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    0%|          | 0/20 [03:36<?, ?it/s]\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "bs= 4, dernière couche sans weight réinit\n",
        "\n",
        "   0%|          | 0/20 [03:36<?, ?it/s]\n",
        "Epoch 0:\n",
        "train loss: 2.0976, valid loss: 1.9806, delta loss: 0.117,\n",
        "acc train: 0.3805, acc valid: 0.3342\n",
        "  5%|▌         | 1/20 [07:40<1:18:14, 247.06s/it]\n",
        "Epoch 1:\n",
        "train loss: 1.5981, valid loss: 1.6424, delta loss: -0.0443,\n",
        "acc train: 0.5566, acc valid: 0.5198\n",
        " 10%|█         | 2/20 [11:44<1:13:35, 245.30s/it]\n",
        "Epoch 2:\n",
        "train loss: 1.3427, valid loss: 1.4754, delta loss: -0.1327,\n",
        "acc train: 0.6323, acc valid: 0.558\n",
        " 15%|█▌        | 3/20 [15:48<1:09:20, 244.76s/it]\n",
        "Epoch 3:\n",
        "train loss: 1.1706, valid loss: 1.3436, delta loss: -0.173,\n",
        "acc train: 0.6741, acc valid: 0.6003\n",
        " 20%|██        | 4/20 [19:52<1:05:11, 244.48s/it]\n",
        "Epoch 4:\n",
        "train loss: 1.0137, valid loss: 1.255, delta loss: -0.2413,\n",
        "acc train: 0.7238, acc valid: 0.6357\n",
        " 25%|██▌       | 5/20 [23:56<1:01:04, 244.29s/it]\n",
        "Epoch 5:\n",
        "train loss: 0.8842, valid loss: 1.2542, delta loss: -0.37,\n",
        "acc train: 0.7601, acc valid: 0.6303\n",
        " 30%|███       | 6/20 [28:00<56:59, 244.26s/it]\n",
        "Epoch 6:\n",
        "train loss: 0.7609, valid loss: 1.1673, delta loss: -0.4064,\n",
        "acc train: 0.7885, acc valid: 0.6821\n",
        " 35%|███▌      | 7/20 [32:04<52:54, 244.19s/it]\n",
        "Epoch 7:\n",
        "train loss: 0.6425, valid loss: 1.1662, delta loss: -0.5236,\n",
        "acc train: 0.8265, acc valid: 0.6726\n",
        " 40%|████      | 8/20 [36:08<48:49, 244.15s/it]\n",
        "Epoch 8:\n",
        "train loss: 0.5414, valid loss: 1.1611, delta loss: -0.6197,\n",
        "acc train: 0.8583, acc valid: 0.6903\n",
        " 45%|████▌     | 9/20 [40:12<44:45, 244.10s/it]\n",
        "Epoch 9:\n",
        "train loss: 0.442, valid loss: 1.1847, delta loss: -0.7427,\n",
        "acc train: 0.8867, acc valid: 0.6739\n",
        " 55%|█████▌    | 11/20 [43:46<34:13, 228.21s/it]\n",
        "Epoch 10:\n",
        "train loss: 0.3472, valid loss: 1.185, delta loss: -0.8377,\n",
        "acc train: 0.9175, acc valid: 0.6876"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJW8taWK2kGf"
      },
      "outputs": [],
      "source": [
        "bs = 4 en changeant les poids des deux dernières couches, avec 0.2 en std\n",
        "\n",
        "train loss: 0.8832, valid loss: 1.2811, delta loss: -0.3979,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21erU3jT7kOB"
      },
      "outputs": [],
      "source": [
        "bs = 4 en changeant les poids des deux dernières couches, avec 0.1 en std\n",
        "\n",
        "train loss: 0.8925, valid loss: 1.2393, delta loss: -0.3468,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqXN3F79y4oY"
      },
      "outputs": [],
      "source": [
        "bs = 4 sans changer les poids de la dernière couche\n",
        "\n",
        "train loss: 0.79, valid loss: 1.2503, delta loss: -0.4603,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ESbR2dhz5dbF"
      },
      "outputs": [],
      "source": [
        "sub = list(map(lambda x: np.array(nn.Softmax(dim=1)(torch.tensor(x))), res))\n",
        "\n",
        "y_pred = np.concatenate(sub, axis=0)\n",
        "y_pred.shape\n",
        "\n",
        "proteins_test = list()\n",
        "with open('data/graph_labels.txt', 'r') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        t = line.split(',')\n",
        "        if len(t[1][:-1]) == 0:\n",
        "            proteins_test.append(t[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uDAIvAQvA6rl"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('8epochs_samedi_uniform_bs2_lr3e5.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    lst = list()\n",
        "    for i in range(18):\n",
        "        lst.append('class'+str(i))\n",
        "    lst.insert(0, \"name\")\n",
        "    writer.writerow(lst)\n",
        "    for i, protein in enumerate(proteins_test):\n",
        "        lst = y_pred[i,:].tolist()\n",
        "        lst.insert(0, protein)\n",
        "        writer.writerow(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdXPgLJqAqeG"
      },
      "outputs": [],
      "source": [
        "Epoch 0:\n",
        "train loss: 1.8907, valid loss: 1.6979,\n",
        "acc train: 0.4579acc valid: 0.485\n",
        "Epoch 1:\n",
        "train loss: 1.3323, valid loss: 1.4126,\n",
        "acc train: 0.622acc valid: 0.5913\n",
        "Epoch 2:\n",
        "train loss: 1.0555, valid loss: 1.2206,\n",
        "acc train: 0.7087acc valid: 0.6335\n",
        "Epoch 3:\n",
        "train loss: 0.8302, valid loss: 1.1845,\n",
        "acc train: 0.7745acc valid: 0.6662\n",
        "Epoch 4:\n",
        "train loss: 0.649, valid loss: 1.2185,\n",
        "acc train: 0.8267acc valid: 0.6499\n",
        "Epoch 5:\n",
        "train loss: 0.4991, valid loss: 1.1734,\n",
        "acc train: 0.8745acc valid: 0.6703\n",
        "Epoch 6:\n",
        "train loss: 0.3834, valid loss: 1.1397,\n",
        "acc train: 0.909acc valid: 0.688\n",
        "Epoch 7:\n",
        "train loss: 0.2994, valid loss: 1.1628,\n",
        "acc train: 0.9351acc valid: 0.6839\n",
        "Epoch 8:\n",
        "train loss: 0.238, valid loss: 1.186,\n",
        "acc train: 0.9553acc valid: 0.6826\n",
        "Early stopping end of epoch 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqC8fhG6AlkX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vX27zXn_V69"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgyKhmKYqP-W"
      },
      "outputs": [],
      "source": [
        "  5%|▌         | 1/20 [03:28<1:05:59, 208.38s/it]\n",
        "Epoch 0:\n",
        "train loss: 1.9055, valid loss: 1.6867,\n",
        "acc train: 0.4484acc valid: 0.4946\n",
        " 10%|█         | 2/20 [06:56<1:02:28, 208.27s/it]\n",
        "Epoch 1:\n",
        "train loss: 1.3533, valid loss: 1.4342,\n",
        "acc train: 0.6246acc valid: 0.5681\n",
        " 15%|█▌        | 3/20 [10:24<59:01, 208.30s/it]  \n",
        "Epoch 2:\n",
        "train loss: 1.0844, valid loss: 1.3051,\n",
        "acc train: 0.6972acc valid: 0.6131\n",
        " 20%|██        | 4/20 [13:52<55:30, 208.18s/it]\n",
        "Epoch 3:\n",
        "train loss: 0.8398, valid loss: 1.248,\n",
        "acc train: 0.7732acc valid: 0.6349\n",
        " 25%|██▌       | 5/20 [17:21<52:02, 208.15s/it]\n",
        "Epoch 4:\n",
        "train loss: 0.6354, valid loss: 1.2457,\n",
        "acc train: 0.8288acc valid: 0.6471\n",
        " 30%|███       | 6/20 [20:49<48:33, 208.13s/it]\n",
        "Epoch 5:\n",
        "train loss: 0.4625, valid loss: 1.1694,\n",
        "acc train: 0.8825acc valid: 0.6662\n",
        " 35%|███▌      | 7/20 [24:17<45:05, 208.15s/it]\n",
        "Epoch 6:\n",
        "train loss: 0.3168, valid loss: 1.1416,\n",
        "acc train: 0.9216acc valid: 0.6771\n",
        " 35%|███▌      | 7/20 [27:45<45:05, 208.15s/it]\n",
        "Epoch 7:\n",
        "train loss: 0.2154, valid loss: 1.1496,\n",
        "acc train: 0.9583acc valid: 0.6798\n",
        " 40%|████      | 8/20 [31:15<46:52, 234.40s/it]\n",
        "Epoch 8:\n",
        "train loss: 0.1402, valid loss: 1.204,\n",
        "acc train: 0.9791acc valid: 0.6812\n",
        "Early stopping end of epoch 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrGYY1dSTQ4G",
        "outputId": "8e7c03a7-ed11-46b5-a1ed-e2634407c3c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = torch.load(\"../gdrive/MyDrive/data/last_model_checkpoint.pt\")\n",
        "model.load_state_dict(checkpoint['state'])\n",
        "model.eval()\n",
        "()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY6Z3fWoZyk9"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl43TbMIaId3"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "os.environ [\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:516\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64TGvGnRa3P9"
      },
      "outputs": [],
      "source": [
        "model = model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "ICI4CdCiUAYL",
        "outputId": "3f72af41-57e2-4a1d-cd26-fdfd77d1e7ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1223 [00:05<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2451dbdaee4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mfeatures_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ec1fe8b1ee6f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         output = self.bert(\n\u001b[0m\u001b[1;32m     13\u001b[0m           \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1021\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 426\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.80 GiB (GPU 0; 39.59 GiB total capacity; 32.94 GiB already allocated; 2.76 GiB free; 35.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "batchSize = 2\n",
        "res = list()\n",
        "\n",
        "indices = np.arange(len(sequences_test))\n",
        "\n",
        "for i in tqdm(range(0, len(sequences_test), int(batchSize/2))):\n",
        "    model.eval()\n",
        "\n",
        "    incr_i = min(i+int(batchSize/2), len(sequences_test))\n",
        "    # indices = shuffle_test[i: incr_i]\n",
        "\n",
        "    # targets_ = torch.tensor(y_valid[indices])\n",
        "    # targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "    \n",
        "    sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_test[indices]])\n",
        "\n",
        "    attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_test[indices]])\n",
        "\n",
        "    # features_ = np.array(features_test)[indices]\n",
        "    # features_ = np.vstack(features_)\n",
        "    # features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "    # cum_nodes = adj_shapes_test[indices]\n",
        "    # idx_batch = np.repeat(np.arange(\n",
        "    #     int(batchSize/4) if incr_i%int(batchSize/4)==0 else incr_i%int(batchSize/4)\n",
        "    #     ), cum_nodes)\n",
        "    # idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "    \n",
        "    # adj_ = adj_test[indices]\n",
        "    # adj_ = sp.block_diag(adj_)\n",
        "    # adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "    output = model(sequences_, attention_mask_)\n",
        "\n",
        "    del features_\n",
        "    del adj_\n",
        "    del idx_batch\n",
        "    del sequences_\n",
        "    del attention_mask_\n",
        "    \n",
        "    res.append(output)\n",
        "\n",
        "    del output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c28YLr3W0jB"
      },
      "outputs": [],
      "source": [
        "arg1 = np.arange(\n",
        "  int(batchSize/4) if incr_i%int(batchSize/4)==0 else incr_i%int(batchSize/4)\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgEyAa3TW2em",
        "outputId": "d124db05-9b2b-432b-94bb-a849ab092225"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1223,)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cum_nodes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv4ikvmnWwsE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3lgq9iDT3aF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JHdVz8RDVZT"
      },
      "outputs": [],
      "source": [
        "  5%|▌         | 1/20 [03:20<1:03:26, 200.36s/it]\n",
        "Epoch 0:\n",
        "train loss: 1.8427, valid loss: 1.6749,\n",
        "acc train: 0.4519acc valid: 0.4986\n",
        " 10%|█         | 2/20 [06:40<1:00:03, 200.17s/it]\n",
        "Epoch 1:\n",
        "train loss: 1.3959, valid loss: 1.4417,\n",
        "acc train: 0.5976acc valid: 0.5938\n",
        " 10%|█         | 2/20 [10:00<1:00:03, 200.17s/it]\n",
        "Epoch 2:\n",
        "train loss: 1.1997, valid loss: 1.4535,\n",
        "acc train: 0.6586acc valid: 0.5978\n",
        " 15%|█▌        | 3/20 [13:21<1:15:40, 267.10s/it]\n",
        "Epoch 3:\n",
        "train loss: 1.046, valid loss: 1.5351,\n",
        "acc train: 0.7006acc valid: 0.5557\n",
        "Early stopping end of epoch 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-brUXnR_m5SK",
        "outputId": "d4ca5600-c65d-41a2-835e-eabebb4bbc4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " 10%|█         | 1/10 [03:22<30:19, 202.12s/it]\n",
        "Epoch 0, train loss: 1.8413, valid loss: 2.0762\n",
        ", acc train: 0.4657acc train: 0.3791\n",
        " 20%|██        | 2/10 [06:44<26:56, 202.10s/it]\n",
        "Epoch 1, train loss: 1.6625, valid loss: 1.7745\n",
        ", acc train: 0.5146acc train: 0.4742\n",
        " 20%|██        | 2/10 [10:06<40:24, 303.07s/it]\n",
        "Epoch 2, train loss: 1.5559, valid loss: 1.8608\n",
        ", acc train: 0.553acc train: 0.5068\n",
        "Early stopping end of epoch 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8StguImHwSOB"
      },
      "outputs": [],
      "source": [
        "10%|█         | 1/10 [03:22<30:19, 202.14s/it]\n",
        "Epoch 0:\n",
        "train loss: 2.9345, valid loss: 2.6558,\n",
        "acc train: 0.2496acc valid: 0.2908\n",
        " 20%|██        | 2/10 [06:44<26:56, 202.11s/it]\n",
        "Epoch 1:\n",
        "train loss: 2.2657, valid loss: 2.4044,\n",
        "acc train: 0.3677acc valid: 0.3152\n",
        " 30%|███       | 3/10 [10:06<23:34, 202.09s/it]\n",
        "Epoch 2:\n",
        "train loss: 1.9986, valid loss: 2.3356,\n",
        "acc train: 0.4311acc valid: 0.3356\n",
        " 40%|████      | 4/10 [13:28<20:12, 202.01s/it]\n",
        "Epoch 3:\n",
        "train loss: 1.8384, valid loss: 1.9591,\n",
        "acc train: 0.4693acc valid: 0.4389\n",
        " 40%|████      | 4/10 [16:50<25:15, 252.53s/it]\n",
        "Epoch 4:\n",
        "train loss: 1.6969, valid loss: 2.1203,\n",
        "acc train: 0.5152acc valid: 0.4524\n",
        "Early stopping end of epoch 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nnLO7dp98O4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6E0c9X_Ar2b"
      },
      "outputs": [],
      "source": [
        "# for batch_num, e in enumerate(get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
        "#                 tokenizer=tokenizer, max_len=600, batch_size=8, shuffle=True, version=version, drop_last=True)):\n",
        "        \n",
        "#         batch_indices = e['indexs']\n",
        "        \n",
        "\n",
        "#         target = F.one_hot(e['target'], 18).float().to(device)\n",
        "\n",
        "#         #Compute sequence forward\n",
        "#         input = e['input_ids'].to(device)\n",
        "#         src_mask = e['attention_mask'].to(device)\n",
        "\n",
        "#         # Compute graph forward\n",
        "#         features_ = np.array(features)[batch_indices]\n",
        "#         features_ = np.vstack(features_)\n",
        "#         features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "#         adj_ = adj[batch_indices]\n",
        "#         adj_ = sp.block_diag(adj_)\n",
        "#         adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "#         cum_nodes = adj_shapes[batch_indices]\n",
        "#         idx_batch = np.repeat(np.arange(\n",
        "#             len(batch_indices)\n",
        "#             ), cum_nodes)\n",
        "#         idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "        \n",
        "#         output = model(features_, adj_, idx_batch, input, src_mask)\n",
        "\n",
        "#         # Backward\n",
        "#         loss = criterion(output, target)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Compute metrics\n",
        "#         epoch_loss.append(loss.item())\n",
        "        \n",
        "#         target = target.to('cpu').detach().numpy()\n",
        "#         output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "#         accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "#         epoch_accuracy.append(accuracy)\n",
        "\n",
        "#     for batch_num, e in enumerate(get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
        "#                 tokenizer=tokenizer, max_len=600, batch_size=8, shuffle=False, version=\"valid\", drop_last=False)):\n",
        "      \n",
        "#         batch_indices = e['indexs']\n",
        "#         target = F.one_hot(e['target'], 18).float().to(device)\n",
        "\n",
        "#         #Compute sequence forward\n",
        "#         input = e['input_ids'].to(device)\n",
        "#         src_mask = e['attention_mask'].to(device)\n",
        "\n",
        "#         # Compute graph forward\n",
        "#         features_ = np.array(features_valid)[batch_indices]\n",
        "#         features_ = np.vstack(features_)\n",
        "#         features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "#         adj_ = adj_valid[batch_indices]\n",
        "#         adj_ = sp.block_diag(adj_)\n",
        "#         adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "#         cum_nodes = adj_shapes_valid[batch_indices]\n",
        "#         idx_batch = np.repeat(np.arange(\n",
        "#             len(batch_indices)\n",
        "#             ), cum_nodes)\n",
        "#         idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "#         output = model(features_, adj_, idx_batch, input, src_mask)\n",
        "\n",
        "#         # Compute metrics\n",
        "#         epoch_loss_valid.append(loss.item())\n",
        "        \n",
        "#         target = target.to('cpu').detach().numpy()\n",
        "#         output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "#         accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "#         epoch_accuracy_valid.append(accuracy)\n",
        "\n",
        "\n",
        "#     scheduler.step()\n",
        "\n",
        "#     epoch_loss = np.array(epoch_loss).mean()\n",
        "#     epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
        "#     epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "#     epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
        "\n",
        "#     tqdm.write(\n",
        "#         '\\n'\n",
        "#         f'Epoch {epoch}, train loss: {round(epoch_loss, 4)}, '\n",
        "#         f'valid loss: {round(epoch_loss_valid, 4)}\\n, acc train: {round(epoch_accuracy, 4)}'\n",
        "#         f'acc train: {round(epoch_accuracy_valid, 4)}'\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NJ6ZPzwBnOz"
      },
      "outputs": [],
      "source": [
        "torch.save({\"state\": model.state_dict(),}, \"1901__5epochs.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv2_W25bBnOz"
      },
      "outputs": [],
      "source": [
        "version = \"valid\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co0DoZjFQVhq"
      },
      "outputs": [],
      "source": [
        "ref = list()\n",
        "with open('data/graph_labels.txt', 'r') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        t = line.split(',')\n",
        "        if len(t[1][:-1]) == 0:\n",
        "            ref.append(True)\n",
        "        else:\n",
        "            ref.append(False)\n",
        "\n",
        "ref = np.array(ref)\n",
        "ref = np.logical_not(ref)\n",
        "ref.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kxESA96QZGD"
      },
      "outputs": [],
      "source": [
        "is_kept = []\n",
        "with open('data/graph_labels.txt', \"r\") as f1:\n",
        "    for line in f1:\n",
        "        s1, s2 = line.strip().split(',')\n",
        "        if len(s2.strip())>0:\n",
        "            is_kept.append(version != \"valid\")\n",
        "        else :\n",
        "            is_kept.append(version == \"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMZNHasvQame"
      },
      "outputs": [],
      "source": [
        "adj, adj_weight, features, edge_features, Flist = load_data()\n",
        "del Flist\n",
        "del edge_features\n",
        "\n",
        "adj = [normalize_adjacency(A, W) for A, W in zip(adj, adj_weight)]\n",
        "adj_shapes = np.array([at.shape[0] for at in adj])\n",
        "adj = [adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))]\n",
        "\n",
        "adj = np.array(adj)[np.array(is_kept)]\n",
        "features = np.array(features, dtype=object)[np.array(is_kept)]\n",
        "adj_shapes = adj_shapes[np.array(is_kept)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPD6aqzHQgIx"
      },
      "outputs": [],
      "source": [
        "len(adj), len(features), len(adj_shapes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67gre-U1Qg62"
      },
      "outputs": [],
      "source": [
        "data_loader = get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
        "                tokenizer=tokenizer, max_len=600, batch_size=8, shuffle=False, version=version, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qzws1nfhBSV"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model.training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdCSLFpVQi6T"
      },
      "outputs": [],
      "source": [
        "sub = []\n",
        "\n",
        "for batch_num, e in tqdm(enumerate(data_loader)):\n",
        "        \n",
        "    batch_indices = e['indexs']\n",
        "\n",
        "    #Compute sequence forward\n",
        "    input = e['input_ids'].to(device)\n",
        "    src_mask = e['attention_mask'].to(device)\n",
        "\n",
        "    # Compute graph forward\n",
        "    features_ = np.array(features)[batch_indices]\n",
        "    features_ = np.vstack(features_)\n",
        "    features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "    adj_ = adj[batch_indices]\n",
        "    adj_ = sp.block_diag(adj_)\n",
        "    adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "    cum_nodes = adj_shapes[batch_indices]\n",
        "    idx_batch = np.repeat(np.arange(\n",
        "        len(batch_indices)\n",
        "        ), cum_nodes)\n",
        "    idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "    \n",
        "    output = model(features_, adj_, idx_batch, input, src_mask)\n",
        "    \n",
        "    sub.append(output.to('cpu').detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsul_-4iBnOz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0kqBhyeTvtV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-sWYXpkUWN-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92P_td8SUAf7"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('5epochs_underfitting.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    lst = list()\n",
        "    for i in range(18):\n",
        "        lst.append('class'+str(i))\n",
        "    lst.insert(0, \"name\")\n",
        "    writer.writerow(lst)\n",
        "    for i, protein in enumerate(proteins_test):\n",
        "        lst = y_pred[i,:].tolist()\n",
        "        lst.insert(0, protein)\n",
        "        writer.writerow(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQxUf-gPTosN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHUl3VoaBnOz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "k = 0\n",
        "\n",
        "résumé = []\n",
        "for k in range(18):\n",
        "    bol = tf.argmax(y_test, axis=1) == k\n",
        "    preds = model(X_test)[bol]\n",
        "    targets = y_test[bol]\n",
        "    accuracy_preds = (tf.argmax(preds, axis=1)==k).numpy().mean()\n",
        "    nll = log_loss(y_true=targets, y_pred=preds)\n",
        "    résumé.append((accuracy_preds, nll))\n",
        "\n",
        "acc = [tup[0] for tup in résumé]\n",
        "nll = [tup[1] for tup in résumé]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, tight_layout=True, figsize=(14, 6))\n",
        "axs[0].bar(np.arange(len(acc)), acc)\n",
        "axs[0].set_title('Accuracy')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "axs[1].bar(np.arange(len(nll)), nll)\n",
        "axs[1].set_title('Negative Log Likelihood')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "axs[2].bar(np.arange(len(nll)), [440.,  50., 939.,  60., 112., 625., 202.,  74., 998.,  57.,  43.,305.,  44.,  59., 548., 226.,  60.,  46.])\n",
        "axs[2].set_title('Pops')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz2189XIBnOz"
      },
      "outputs": [],
      "source": [
        "#GTN version brouillon\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "a = torch.ones(25, 300)\n",
        "b = torch.ones(22, 300)\n",
        "c = torch.ones(15, 300)\n",
        "pad_sequence([a, b, c]).size()\n",
        "\n",
        "\n",
        "\n",
        "from graph_transformer_pytorch import GraphTransformer\n",
        "\n",
        "model = GraphTransformer(\n",
        "    dim = 256,\n",
        "    depth = 6,\n",
        "    edge_dim = 512,             # optional - if left out, edge dimensions is assumed to be the same as the node dimensions above\n",
        "    with_feedforwards = True,   # whether to add a feedforward after each attention layer, suggested by literature to be needed\n",
        "    gated_residual = True,      # to use the gated residual to prevent over-smoothing\n",
        "    rel_pos_emb = True          # set to True if the nodes are ordered, default to False\n",
        ")\n",
        "\n",
        "nodes = torch.randn(1, 128, 256)\n",
        "edges = torch.randn(1, 128, 128, 512)\n",
        "mask = torch.ones(1, 128).bool()\n",
        "\n",
        "nodes, edges = model(nodes, edges, mask = mask)\n",
        "\n",
        "nodes.shape # (1, 128, 256) - project to R^3 for coordinates"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "u72E7pCrBnOw"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "altegrad_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "93201184ae5283544afdd58677953ee734bf67b299385e7b22daff49378f4f38"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12bfd59b85bf4b068665020cfd0a934d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d607a63ac1f9465ca48d85696f0c6ec8",
              "IPY_MODEL_81d7dbdc10ab491485baa5ee11568705",
              "IPY_MODEL_d0d5713810b24e06a971f51a5f1d957f"
            ],
            "layout": "IPY_MODEL_3c90639f8f3a4c84856ae48893e542dd"
          }
        },
        "17c8bbaf3b9e49708dd87a882f697f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19488068a7ec4d94967be6e3f04b95d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257f5aa5336b4c2487462de36fbd5921": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ed55a13cda451396c20c0624440dcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28b01989f7e04de5948f2b78706650ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e6c121bc7c549158dc9c048b60b7234",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f88b7d13d9b640309aa4662a9a28993e",
            "value": 112
          }
        },
        "3200dd4f52484afd9d397218eeaccd26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "341b52c1046e4ca8a45024348e3f3c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c90639f8f3a4c84856ae48893e542dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e6c121bc7c549158dc9c048b60b7234": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45458a80fa5a4f058cd05dab4061ad6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd6933c3e25949f18b9930c09133ac8d",
            "max": 589,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_488be0d55b2d46f1af04cf01c9a6cda7",
            "value": 589
          }
        },
        "488be0d55b2d46f1af04cf01c9a6cda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49423cc17b01474e819b9a03e6e9a9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53552a1add4844b1aa254cf97f359aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e2069e252e44178624103d4bac9037",
            "max": 86,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_341b52c1046e4ca8a45024348e3f3c1c",
            "value": 86
          }
        },
        "573bd8fd266c40d387b07d064f3bde72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8ffa6e98a37409eb66eee1879ba180e",
            "placeholder": "​",
            "style": "IPY_MODEL_8d497159d5c644ab93e914bc94eda822",
            "value": " 589/589 [00:00&lt;00:00, 43.9kB/s]"
          }
        },
        "5c49923877ec457ca0dcbc37d17dfb76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae37cbd255844fecb73f0e20671501ee",
            "placeholder": "​",
            "style": "IPY_MODEL_e9bee3614d0b44888ed357583a4cdcca",
            "value": " 112/112 [00:00&lt;00:00, 8.89kB/s]"
          }
        },
        "7ed0a918143242dda018f84ed93caa4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfecb7c6212d44539b79d42e4f887b5f",
            "placeholder": "​",
            "style": "IPY_MODEL_83288d42e5534a8aacb998b7d6d5f651",
            "value": "Downloading: 100%"
          }
        },
        "819e2fb14dfb46beac0bad721e8aa6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81d7dbdc10ab491485baa5ee11568705": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aea5bd8f08a54a9190e1b7160312353a",
            "max": 80,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b44fd31d6b51446398370f565020139d",
            "value": 80
          }
        },
        "83288d42e5534a8aacb998b7d6d5f651": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d497159d5c644ab93e914bc94eda822": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e17c823ad624339954e5fd87641059f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1e24dad435d4f6b8092d3ffbf456f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a28e039920664a7abbd108eda586e0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b2e6b1b57c4456a075a9705a8e4ae6",
            "placeholder": "​",
            "style": "IPY_MODEL_f8a725f7e71d47ccb599c6338f3bc5c5",
            "value": "Downloading: 100%"
          }
        },
        "a8f1983b26da497da540f53104c0b26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a28e039920664a7abbd108eda586e0ba",
              "IPY_MODEL_53552a1add4844b1aa254cf97f359aef",
              "IPY_MODEL_f593ab09239f43b9b09492dbe4785927"
            ],
            "layout": "IPY_MODEL_819e2fb14dfb46beac0bad721e8aa6f0"
          }
        },
        "ae37cbd255844fecb73f0e20671501ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea5bd8f08a54a9190e1b7160312353a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b44fd31d6b51446398370f565020139d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8ffa6e98a37409eb66eee1879ba180e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd6933c3e25949f18b9930c09133ac8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d5713810b24e06a971f51a5f1d957f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3200dd4f52484afd9d397218eeaccd26",
            "placeholder": "​",
            "style": "IPY_MODEL_d9c6fb18a7754d5f96aff226ff39255d",
            "value": " 80.0/80.0 [00:00&lt;00:00, 5.76kB/s]"
          }
        },
        "d1e2069e252e44178624103d4bac9037": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d607a63ac1f9465ca48d85696f0c6ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_257f5aa5336b4c2487462de36fbd5921",
            "placeholder": "​",
            "style": "IPY_MODEL_a1e24dad435d4f6b8092d3ffbf456f9b",
            "value": "Downloading: 100%"
          }
        },
        "d661bddd1e8b429e9c4672200e7fedd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0123564ce2c4dd9a213ab35589ab683",
              "IPY_MODEL_28b01989f7e04de5948f2b78706650ae",
              "IPY_MODEL_5c49923877ec457ca0dcbc37d17dfb76"
            ],
            "layout": "IPY_MODEL_9e17c823ad624339954e5fd87641059f"
          }
        },
        "d9c6fb18a7754d5f96aff226ff39255d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfecb7c6212d44539b79d42e4f887b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9bee3614d0b44888ed357583a4cdcca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0123564ce2c4dd9a213ab35589ab683": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f516161f956d46fbb8230c0298781bdc",
            "placeholder": "​",
            "style": "IPY_MODEL_25ed55a13cda451396c20c0624440dcc",
            "value": "Downloading: 100%"
          }
        },
        "f516161f956d46fbb8230c0298781bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f593ab09239f43b9b09492dbe4785927": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19488068a7ec4d94967be6e3f04b95d4",
            "placeholder": "​",
            "style": "IPY_MODEL_17c8bbaf3b9e49708dd87a882f697f5d",
            "value": " 86.0/86.0 [00:00&lt;00:00, 6.16kB/s]"
          }
        },
        "f6b2e6b1b57c4456a075a9705a8e4ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f76bc551a8bf4e9a8dd081c1aa4ddf0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ed0a918143242dda018f84ed93caa4e",
              "IPY_MODEL_45458a80fa5a4f058cd05dab4061ad6d",
              "IPY_MODEL_573bd8fd266c40d387b07d064f3bde72"
            ],
            "layout": "IPY_MODEL_49423cc17b01474e819b9a03e6e9a9bb"
          }
        },
        "f88b7d13d9b640309aa4662a9a28993e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8a725f7e71d47ccb599c6338f3bc5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
