{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRzrLyDQBwLF",
        "outputId": "b69e6015-662d-4860-fcf9-3155a3eb9266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'zetaFold' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Mounted at /content/gdrive/\n",
            "/content/zetaFold\n"
          ]
        }
      ],
      "source": [
        "## COLAB AREA\n",
        "\n",
        "!git clone https://github.com/AugustinCombes/zetaFold.git\n",
        "%pip install transformers\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "%cd zetaFold/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zjBShNTJBnOn"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange, tqdm\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "from utils import normalize_adjacency, sparse_mx_to_torch_sparse_tensor, load_data, submit_predictions\n",
        "from dataloader import get_loader\n",
        "from loss import criterion as balanced_criterion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../gdrive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoN9Vaoy-TDd",
        "outputId": "292cb291-1fcb-4754-a3cc-0d1d93898e96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5hufD8Q2BnOq"
      },
      "outputs": [],
      "source": [
        "n_labels = 4888\n",
        "treshold_valid = 733\n",
        "\n",
        "ref = dict()\n",
        "with open('data/graph_labels.txt', 'r') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        t = line.split(',')\n",
        "        if len(t[1][:-1]) != 0:\n",
        "            if len([_ for _ in ref.values() if _ == \"train\"]) < n_labels - treshold_valid:\n",
        "              ref[i] = \"train\"\n",
        "            else :\n",
        "              ref[i] = \"valid\"\n",
        "        else:\n",
        "            ref[i] = \"test\"\n",
        "\n",
        "ref_train = np.array([i for i in range(len(ref)) if ref[i]==\"train\"])\n",
        "ref_valid = np.array([i for i in range(len(ref)) if ref[i]==\"valid\"])\n",
        "ref_test = np.array([i for i in range(len(ref)) if ref[i]==\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_H9LliJSBnOr"
      },
      "outputs": [],
      "source": [
        "device = 'cpu'\n",
        "# device = 'mps'\n",
        "device = 'cuda'\n",
        "\n",
        "version = \"train\"\n",
        "# version = \"valid\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nNXFYx-q3Teo"
      },
      "outputs": [],
      "source": [
        "y = []\n",
        "valid_id = []\n",
        "\n",
        "with open('data/graph_labels.txt', \"r\") as f1:\n",
        "  for line in f1:\n",
        "    s1, s2 = line.strip().split(',')\n",
        "    if len(s2.strip())>0:\n",
        "      y.append(int(s2))\n",
        "    else :\n",
        "      valid_id.append(s1)\n",
        "\n",
        "y = np.array(y)\n",
        "y_train, y_valid = y[:-treshold_valid], y[-treshold_valid:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfo59lxfBnOt"
      },
      "source": [
        "# SEQUENCES ONLY : Finetune DistillProtbert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s2FWjH493mN9"
      },
      "outputs": [],
      "source": [
        "sequences = [] \n",
        "with open('data/sequences.txt', \"r\") as f1:\n",
        "  for line in f1:\n",
        "    sequences.append(' '.join(list(line[:-1])))\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "sequences_train, sequences_valid, sequences_test = sequences[ref_train], sequences[ref_valid], sequences[ref_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "c5f0d9bfaab145eb9319b44ca5dd5b28",
            "b5fb6925a94a4fc2a44166bffb05badd",
            "59c0f7e8359348f39f35aa6f905844e0",
            "b01ace8ca5794f769970c8b91ddefc18",
            "b9171da9d1654ae5b651797bedae1551",
            "8ce38275890943e99e0a8cb08ef36538",
            "50a6b3e896464727b935c343fab1ee18",
            "a027429cb73842e89a6f317c08c14a0a",
            "ae64b29f95634c78be5b4c0c9402d929",
            "f2b56add8c2e46edb7c3c367191843f6",
            "f307e6b4de7245b68cf60d29a5000a3a",
            "2c95b2020a3e48228a4be9bbbe41f88d",
            "d10fec524697410c9ad8bf81c0b9e02a",
            "81f63e2edc3e490dbaa2a4df2cf54f05",
            "a887248da4da4e7ab1b291825de0641b",
            "de11022356034ea5b59edd46a3205778",
            "d00fbf37f607422ca5b818f7671d2b3a",
            "3c4a0e9f989643cfb9d226b600d1bf1a",
            "978b6af3c63543538f33693e031b0b6e",
            "8e3172c1e7434e42a7629db1fc1dc5ac",
            "bce78acdcb4c4edfa427e87ca5b142e6",
            "4c8b61a4c9be4c50acd44088b66adef4",
            "2f16d7844da74e90a657f4afb0673e44",
            "babd46a6665b42e19a047c621e2ecd53",
            "6b811e436583474cb46dedaab5756aa6",
            "09ebdee1d767447e892a1abbe47dfdd0",
            "f45ad67ced524b7ba52d80eb6bcde632",
            "800d592ff7364e088a726ce8db18678c",
            "ef8def30b5c84a959d5942090835ea72",
            "4a933ba3c2074bd38ce98127e5164839",
            "bf3b2f3d33454437806b7d6ebefa3377",
            "02dae53c12b14c73b8dda3d32cdca434",
            "237be4600db94aec90358340807d53c9",
            "5a5751f01a1745a7b3c526d94f52d2ca",
            "578dfbffc17c479fa77030e2d4fd4fb6",
            "09b37607b156462982e5a9ea91feca71",
            "ce102f78889648c6b6ac8824caf7d08e",
            "beb4aa6dc44745c0a8b442f34fa84d3b",
            "c71a6d8d84ae48818d21ed2fdafbc3c0",
            "591b1c207c5f4958b71d653c320fc8ec",
            "2c3338aaecdf4c569329b9792588a924",
            "9f660e17d27c4ce8a5bd151ef911d10d",
            "70bb5a0a868d4dc5b78a84cbba6f6bcd",
            "12e364868dc34945983141abac01bdb9"
          ]
        },
        "id": "X7GTZu6DBnOu",
        "outputId": "e4bbe38e-0c76-4e34-a679-2e86d71d2e56"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5f0d9bfaab145eb9319b44ca5dd5b28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c95b2020a3e48228a4be9bbbe41f88d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f16d7844da74e90a657f4afb0673e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/589 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a5751f01a1745a7b3c526d94f52d2ca"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = 'yarongef/DistilProtBert'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ynVmvgPIBnOu"
      },
      "outputs": [],
      "source": [
        "## Define probert-based classifier\n",
        " \n",
        "class ProteinClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(ProteinClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME).to(device)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
        "        output = self.bert(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask\n",
        "        )\n",
        "        output = output.last_hidden_state[:, 0, :]\n",
        "        output = nn.Dropout(0.5)(output) # 0.3 meilleure submission\n",
        "        output = nn.ReLU()(output)\n",
        "        output = self.classifier(output)\n",
        "        # output = nn.ReLU()(output)\n",
        "        # return output\n",
        "\n",
        "        return nn.LogSoftmax(dim=1)(output)\n",
        "\n",
        "# model = ProteinClassifier(18).to(device)\n",
        "\n",
        "# for module in model.bert.encoder.layer[0:-1]:\n",
        "#     for param in module.parameters():\n",
        "#         param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u72E7pCrBnOw"
      },
      "source": [
        "# GRAPHE : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm1PxVJVBnOx"
      },
      "outputs": [],
      "source": [
        "## Prepare graph features\n",
        "\n",
        "# 1. Full graph-related database in arrays\n",
        "\n",
        "adj, adj_weight, features, edge_features, Flist = load_data()\n",
        "del Flist\n",
        "del edge_features\n",
        "\n",
        "adj = [normalize_adjacency(A, W) for A, W in zip(adj, adj_weight)]\n",
        "adj_shapes = np.array([at.shape[0] for at in adj])\n",
        "adj = np.array([adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))])\n",
        "\n",
        "features = np.array(features, dtype=object)\n",
        "\n",
        "adj_train, adj_valid, adj_test = adj[ref_train], adj[ref_valid], adj[ref_test]\n",
        "features_train, features_valid, features_test = features[ref_train], features[ref_valid], features[ref_test]\n",
        "adj_shapes_train, adj_shapes_valid, adj_shapes_test = adj_shapes[ref_train], adj_shapes[ref_valid], adj_shapes[ref_test]\n",
        "\n",
        "\n",
        "# features = features[:n_labels] if version != \"valid\" else features[n_labels:]\n",
        "# adj = adj[:n_labels] if version != \"valid\" else adj[n_labels:]\n",
        "\n",
        "## 2. Then, we load batchs thanks to \"indexs\" key of dataloader element\n",
        "\n",
        "## Example : loading only the first batch of graph-related data\n",
        "# for e in data_loader:\n",
        "#     break\n",
        "# batch_indices = e['indexs'] ## Indexs that are part of the batch\n",
        "\n",
        "# features_ = np.array(features)[batch_indices]\n",
        "# features_ = np.vstack(features_)\n",
        "# features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "# adj_ = adj[batch_indices]\n",
        "# adj_ = sp.block_diag(adj_)\n",
        "# adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "# cum_nodes = adj_shapes[batch_indices]\n",
        "# idx_batch = np.repeat(np.arange(\n",
        "#     len(batch_indices)\n",
        "#     ), cum_nodes)\n",
        "# idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "\n",
        "# # With GNN model\n",
        "\n",
        "# model = GNN(...).to(device)\n",
        "# output = model(features_, adj_, idx_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqcb2p5d8XAO"
      },
      "outputs": [],
      "source": [
        "## Prepare graph features\n",
        "\n",
        "# 1. Full graph-related database in arrays\n",
        "\n",
        "adj, adj_weight, features, edge_features, Flist = load_data()\n",
        "del Flist\n",
        "del edge_features\n",
        "\n",
        "adj = [normalize_adjacency(A, W) for A, W in zip(adj, adj_weight)]\n",
        "adj_shapes = np.array([at.shape[0] for at in adj])\n",
        "adj = np.array([adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))])\n",
        "\n",
        "features = np.array(features, dtype=object)\n",
        "\n",
        "adj_train, adj_valid, adj_test = adj[ref_train], adj[ref_valid], adj[ref_test]\n",
        "features_train, features_valid, features_test = features[ref_train], features[ref_valid], features[ref_test]\n",
        "adj_shapes_train, adj_shapes_valid, adj_shapes_test = adj_shapes[ref_train], adj_shapes[ref_valid], adj_shapes[ref_test]\n",
        "\n",
        "\n",
        "# features = features[:n_labels] if version != \"valid\" else features[n_labels:]\n",
        "# adj = adj[:n_labels] if version != \"valid\" else adj[n_labels:]\n",
        "\n",
        "## 2. Then, we load batchs thanks to \"indexs\" key of dataloader element\n",
        "\n",
        "## Example : loading only the first batch of graph-related data\n",
        "# for e in data_loader:\n",
        "#     break\n",
        "# batch_indices = e['indexs'] ## Indexs that are part of the batch\n",
        "\n",
        "shuffle_train = np.arange(len(sequences_train))\n",
        "indices = shuffle_train[:32]\n",
        "\n",
        "features_ = np.array(features_train)[indices]\n",
        "features_ = np.vstack(features_)\n",
        "features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "adj_ = adj_train[indices]\n",
        "adj_ = sp.block_diag(adj_)\n",
        "adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "cum_nodes = adj_shapes_train[indices]\n",
        "idx_batch = np.repeat(np.arange(\n",
        "    len(indices)\n",
        "    ), cum_nodes)\n",
        "idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "\n",
        "# # With GNN model\n",
        "\n",
        "# model = GNN(...).to(device)\n",
        "# output = model(features_, adj_, idx_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPwK1BfS8XAO",
        "outputId": "a5908b61-41f9-4a3d-e1bf-773cdee65fd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8221, 86])"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# OLD\n",
        "\n",
        "# features_ = np.array(features_train)[indices]\n",
        "# features_ = np.vstack(features_)\n",
        "# features_ = torch.FloatTensor(features_).to(device)\n",
        "# features_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr_12y4J8XAP",
        "outputId": "dc238b8f-afab-44c5-c889-d53b834e5419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8221, 86])\n",
            "74646\n"
          ]
        }
      ],
      "source": [
        "# NEW\n",
        "\n",
        "# layer = nn.TransformerEncoderLayer(d_model=86, nhead=2, dim_feedforward=256, batch_first=True)\n",
        "\n",
        "# features_ = np.array(features_train)[indices]\n",
        "# features_ = torch.nn.utils.rnn.pad_sequence(\n",
        "#     [torch.tensor(f, dtype=torch.float32) for f in features_],\n",
        "#     batch_first=True\n",
        "#     )\n",
        "# mask = torch.nn.utils.rnn.pad_sequence([torch.zeros(e) for e in cum_nodes], batch_first=True, padding_value=1).type(torch.bool)#\n",
        "# features_ = layer(features_, src_key_padding_mask=mask)\n",
        "# features_ = torch.vstack([features_[j,:c] for j,c in enumerate(cum_nodes)])\n",
        "# print(features_.shape)\n",
        "\n",
        "# for k,v in layer.state_dict().items():\n",
        "#     # print(k, v.shape)\n",
        "#     ()\n",
        "# 258*86+258+86*86+86+256*86+256+86*256+86*5    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpIToMxiBnOx"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple message passing model that consists of 2 message passing layers\n",
        "    and the sum aggregation function\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super(GNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # self.fc2 = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8)\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, n_class)\n",
        "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x_in, adj, idx):\n",
        "        # first message passing layer\n",
        "        x = self.fc1(x_in)\n",
        "        x = self.relu(torch.mm(adj, x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # second message passing layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(torch.mm(adj, x))\n",
        "        \n",
        "        # sum aggregator\n",
        "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
        "        out = torch.zeros(torch.max(idx)+1, x.size(1)).to(x_in.device)\n",
        "        out = out.scatter_add_(0, idx, x)\n",
        "        \n",
        "        # batch normalization layer\n",
        "        out = self.bn(out)\n",
        "\n",
        "        # mlp to produce output\n",
        "        out = self.relu(self.fc3(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc4(out)\n",
        "        \n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "model = GNN(86, 64, 0.2, 18).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be4a9fxJ8XAR"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple message passing model that consists of 2 message passing layers\n",
        "    and the sum aggregation function\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
        "        super(GNN, self).__init__()\n",
        "        # self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc1 = nn.TransformerEncoderLayer(d_model=86, nhead=2, dim_feedforward=256, batch_first=True)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        \n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, n_class)\n",
        "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x_in, adj, idx, cum_nodes):\n",
        "        # first message passing layer\n",
        "        pad_mask = torch.nn.utils.rnn.pad_sequence([torch.zeros(e) for e in cum_nodes], batch_first=True, padding_value=1).type(torch.bool)\n",
        "        x = self.fc1(x_in, src_key_padding_mask=pad_mask)\n",
        "        print('1', x.shape)\n",
        "        x = torch.vstack([x[j,:c] for j,c in enumerate(cum_nodes)])\n",
        "        print('2', x.shape)\n",
        "        x = self.relu(torch.mm(adj, x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # second message passing layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(torch.mm(adj, x))\n",
        "        \n",
        "        # sum aggregator\n",
        "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
        "        out = torch.zeros(torch.max(idx)+1, x.size(1)).to(x_in.device)\n",
        "        out = out.scatter_add_(0, idx, x)\n",
        "        \n",
        "        # batch normalization layer\n",
        "        out = self.bn(out)\n",
        "\n",
        "        # mlp to produce output\n",
        "        out = self.relu(self.fc3(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc4(out)\n",
        "        \n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "model = GNN(86, 64, 0.2, 18).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeHW1IQK8XAR",
        "outputId": "5805552e-2d51-4a9c-a3a5-e79249d9cfe9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 torch.Size([64, 796, 86])\n",
            "2 torch.Size([14822, 86])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (14822x86 and 64x64)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[133], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m adj_ \u001b[39m=\u001b[39m sparse_mx_to_torch_sparse_tensor(adj_)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     62\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 63\u001b[0m output \u001b[39m=\u001b[39m model(features_, adj_, idx_batch, cum_nodes)\n\u001b[1;32m     65\u001b[0m \u001b[39mdel\u001b[39;00m features_\n\u001b[1;32m     66\u001b[0m \u001b[39mdel\u001b[39;00m adj_\n",
            "File \u001b[0;32m~/Desktop/envs/altegrad_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[132], line 29\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x_in, adj, idx, cum_nodes)\u001b[0m\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     28\u001b[0m \u001b[39m# second message passing layer\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x)\n\u001b[1;32m     30\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(torch\u001b[39m.\u001b[39mmm(adj, x))\n\u001b[1;32m     32\u001b[0m \u001b[39m# sum aggregator\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/envs/altegrad_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1423\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1421\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1422\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1424\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Desktop/envs/altegrad_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (14822x86 and 64x64)"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), \n",
        "    # lr=1,\n",
        "    lr=1e-3,\n",
        "    # weight_decay=0.0001\n",
        "    )\n",
        "epochs = 50\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
        "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-4) * ((epoch - epochs)/ epochs))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "shuffle_train = np.arange(len(sequences_train))\n",
        "shuffle_valid = np.arange(len(sequences_valid))\n",
        "\n",
        "batchSize = 64\n",
        "\n",
        "patience = 50\n",
        "patience_count = 0\n",
        "\n",
        "previous_epoch_loss_valid = 300\n",
        "\n",
        "res = list()\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    epoch_loss, epoch_loss_valid, epoch_accuracy, epoch_accuracy_valid = [], [], [], []\n",
        "\n",
        "    np.random.shuffle(shuffle_train)\n",
        "    np.random.shuffle(shuffle_valid)\n",
        "\n",
        "\n",
        "    # Train\n",
        "    # batchSize=2\n",
        "    model.train()\n",
        "    for i in range(0, len(sequences_train), batchSize):\n",
        "\n",
        "        incr_i = min(i+batchSize, len(sequences_train))\n",
        "        indices = shuffle_train[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_train[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "        \n",
        "        # features_ = np.array(features_train)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "        features_ = np.array(features_train)[indices]\n",
        "        features_ = torch.nn.utils.rnn.pad_sequence(\n",
        "            [torch.tensor(f, dtype=torch.float32) for f in features_],\n",
        "            batch_first=True\n",
        "            ).to(device)\n",
        "\n",
        "        cum_nodes = adj_shapes_train[indices]\n",
        "        idx_batch = np.repeat(np.arange(\n",
        "            batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
        "            ), cum_nodes)\n",
        "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        adj_ = adj_train[indices]\n",
        "        adj_ = sp.block_diag(adj_)\n",
        "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features_, adj_, idx_batch, cum_nodes)\n",
        "        \n",
        "        del features_\n",
        "        del adj_\n",
        "        del idx_batch\n",
        "\n",
        "        # Backward\n",
        "        loss = criterion(output, targets_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute metrics\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # Validation\n",
        "    # batchSize=64\n",
        "    model.eval()\n",
        "    for i in range(0, len(sequences_valid), int(batchSize)):\n",
        "\n",
        "        incr_i = min(i+int(batchSize), len(sequences_valid))\n",
        "        indices = shuffle_valid[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_valid[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "\n",
        "        # features_ = np.array(features_train)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "        features_ = np.array(features_train)[indices]\n",
        "        features_ = torch.nn.utils.rnn.pad_sequence(\n",
        "            [torch.tensor(f, dtype=torch.float32) for f in features_],\n",
        "            batch_first=True\n",
        "            ).to(device)\n",
        "\n",
        "        cum_nodes = adj_shapes_valid[indices]\n",
        "        idx_batch = np.repeat(np.arange(\n",
        "            int(batchSize) if incr_i%int(batchSize)==0 else incr_i%int(batchSize)\n",
        "            ), cum_nodes)\n",
        "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        adj_ = adj_valid[indices]\n",
        "        adj_ = sp.block_diag(adj_)\n",
        "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        output = model(features_, adj_, idx_batch, cum_nodes)\n",
        "\n",
        "        del features_\n",
        "        del adj_\n",
        "        del idx_batch\n",
        "\n",
        "        # Compute metrics\n",
        "        loss = criterion(output, targets_)\n",
        "        epoch_loss_valid.append(loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy_valid.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "    epoch_loss = np.array(epoch_loss).mean()\n",
        "    epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
        "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "    epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
        "\n",
        "    tqdm.write(\n",
        "        '\\n'\n",
        "        f'Epoch {epoch}:\\ntrain loss: {round(epoch_loss, 4)}, '\n",
        "        f'valid loss: {round(epoch_loss_valid, 4)}, delta loss: {round(epoch_loss-epoch_loss_valid, 4)},\\nacc train: {round(epoch_accuracy, 4)}, '\n",
        "        f'acc valid: {round(epoch_accuracy_valid, 4)}'\n",
        "        )\n",
        "    \n",
        "    # Early stopping :\n",
        "    if previous_epoch_loss_valid < epoch_loss_valid:\n",
        "        if patience_count == 0:\n",
        "          torch.save({\"state\": model.state_dict(),}, \"last_model_checkpoint.pt\")\n",
        "        patience_count +=1\n",
        "        if patience_count == patience:\n",
        "          print(f'Early stopping end of epoch {epoch}')\n",
        "          break\n",
        "\n",
        "    else :\n",
        "        previous_epoch_loss_valid = epoch_loss_valid\n",
        "        patience_count = 0\n",
        "\n",
        "        # Save last best results\n",
        "        del res\n",
        "        res = list()\n",
        "        shuffle_test = np.arange(len(sequences_test))\n",
        "\n",
        "        for i in range(0, len(sequences_test), int(batchSize)):\n",
        "            model.eval()\n",
        "\n",
        "            incr_i = min(i+int(batchSize), len(sequences_test))\n",
        "            indices = shuffle_test[i: incr_i]\n",
        "\n",
        "            features_ = np.array(features_test)[indices]\n",
        "            features_ = np.vstack(features_)\n",
        "            features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "            cum_nodes = adj_shapes_test[indices]\n",
        "            idx_batch = np.repeat(np.arange(\n",
        "                batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
        "                ), cum_nodes)\n",
        "            idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "            \n",
        "            adj_ = adj_test[indices]\n",
        "            adj_ = sp.block_diag(adj_)\n",
        "            adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "            output = model(features_, adj_, idx_batch)\n",
        "\n",
        "            res.append(output.to('cpu').detach().numpy())\n",
        "\n",
        "            del output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RYxyeDJ8XAT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "k = 0\n",
        "\n",
        "résumé = []\n",
        "for k in range(18):\n",
        "    bol = tf.argmax(y_test, axis=1) == k\n",
        "    preds = model(X_test)[bol]\n",
        "    targets = y_test[bol]\n",
        "    accuracy_preds = (tf.argmax(preds, axis=1)==k).numpy().mean()\n",
        "    nll = log_loss(y_true=targets, y_pred=preds)\n",
        "    résumé.append((accuracy_preds, nll))\n",
        "\n",
        "acc = [tup[0] for tup in résumé]\n",
        "nll = [tup[1] for tup in résumé]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, tight_layout=True, figsize=(14, 6))\n",
        "axs[0].bar(np.arange(len(acc)), acc)\n",
        "axs[0].set_title('Accuracy')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "axs[1].bar(np.arange(len(nll)), nll)\n",
        "axs[1].set_title('Negative Log Likelihood')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "axs[2].bar(np.arange(len(nll)), [440.,  50., 939.,  60., 112., 625., 202.,  74., 998.,  57.,  43.,305.,  44.,  59., 548., 226.,  60.,  46.])\n",
        "axs[2].set_title('Pops')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CZkLR2l8XAT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "1F1E96OP9ICA",
        "outputId": "22841bb5-d726-4d39-9154-6093f59acfee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4888 4888 4888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8, cel: 2.3809, nll: 2.3809, acc: 0.2111:  18%|█▊        | 9/50 [00:46<03:32,  5.19s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-898958f36e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0madj_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0madj_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_diag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0madj_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_mx_to_torch_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mblock_diag\u001b[0;34m(mats, format, dtype)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtocoo\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         return coo_matrix((self.data, (row, col)), self.shape, copy=copy,\n\u001b[0m\u001b[1;32m   1032\u001b[0m                           dtype=self.dtype)\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row index exceeds matrix dimensions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     38\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     39\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = GNN(86, 64, 0.1, 18).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "epochs = 50\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
        "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-3) * ((epoch - epochs)/ epochs))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "data_loader = get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
        "                tokenizer=tokenizer, max_len=600, batch_size=64, shuffle=False, version=version, drop_last=True)\n",
        "                \n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    epoch_loss, epoch_log_loss, epoch_accuracy = [], [], []\n",
        "\n",
        "    for batch_num, e in enumerate(data_loader):\n",
        "        \n",
        "        batch_indices = e['indexs']\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target = F.one_hot(e['target'], 18).float().to(device)\n",
        "\n",
        "        # Compute graph forward\n",
        "        features_ = np.array(features)[batch_indices]\n",
        "        features_ = np.vstack(features_)\n",
        "        features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "        adj_ = adj[batch_indices]\n",
        "        adj_ = sp.block_diag(adj_)\n",
        "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        cum_nodes = adj_shapes[batch_indices]\n",
        "        idx_batch = np.repeat(np.arange(\n",
        "            len(batch_indices)\n",
        "            ), cum_nodes)\n",
        "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        output = model(features_, adj_, idx_batch)\n",
        "\n",
        "        # Backward\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute metrics\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        target = target.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "        nll_loss = log_loss(target, output)\n",
        "        epoch_log_loss.append(nll_loss)\n",
        "\n",
        "        accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy.append(accuracy)\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "    epoch_loss = np.array(epoch_loss).mean()\n",
        "    epoch_log_loss = np.array(epoch_log_loss).mean()\n",
        "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "\n",
        "    pbar.set_description(\n",
        "        f'Epoch {epoch}, cel: {round(epoch_loss, 4)}, '\n",
        "        f'nll: {round(epoch_log_loss, 4)}, acc: {round(epoch_accuracy, 4)}'\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhlE2OVR-DYF"
      },
      "outputs": [],
      "source": [
        "torch.save({\"state\": model.state_dict(),}, \"graphe_pretrain.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x34ufcZYBnOy"
      },
      "source": [
        "# MULTICULTURAL MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUyExhp-BnOy",
        "outputId": "81011bad-217a-40e1-e9e9-05ada407ec14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at yarongef/DistilProtBert were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at yarongef/DistilProtBert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# class ConcatModel(nn.Module):\n",
        "#     def __init__(self, out_dim, feature_dim, graph_dim):\n",
        "#         super(ConcatModel, self).__init__()\n",
        "#         # self.seqModel = ProteinClassifier(out_dim).to(device)\n",
        "#         # self.graphModel = GNN(feature_dim, graph_dim, dropout=0.25, n_class=out_dim).to(device)\n",
        "\n",
        "#         self.seqModel = ProteinClassifier(18).to(device)\n",
        "\n",
        "#         # self.classifier = nn.Linear(out_dim, 18)\n",
        "#         # self.activation = nn.LogSoftmax(dim=1)\n",
        "\n",
        "#     def forward(self, x_in, adj, idx, input_ids, attention_mask):\n",
        "#         seqEmbedding = self.seqModel(input_ids, attention_mask)\n",
        "#         # graphEmbedding = self.graphModel(x_in, adj, idx)\n",
        "        \n",
        "#         # out = torch.concat([graphEmbedding, seqEmbedding], dim=1)\n",
        "        \n",
        "#         # out = self.classifier(out)\n",
        "        \n",
        "#         # return self.activation(out)\n",
        "#         return nn.LogSoftmax(dim=1)(seqEmbedding)\n",
        "\n",
        "# model = ConcatModel(64, 86, 128).to(device)\n",
        "model = ProteinClassifier(18).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ylUrqjWXBnOy"
      },
      "outputs": [],
      "source": [
        "for module in model.bert.encoder.layer[:-1]:\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "for module in model.bert.encoder.layer[-1:]:\n",
        "    # module._modules[\"output\"].dense.weight.data.normal_(mean=0.0, std=0.1)\n",
        "    module._modules[\"output\"].dense.weight.data.uniform_(-0.1, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "VE3AFyv87MAh"
      },
      "outputs": [],
      "source": [
        "encode = lambda s : tokenizer.encode_plus(\n",
        "            s,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=989,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pops = np.array([440.,  50., 939.,  60., 112., 625., 202.,  74., 998.,  57.,  43.,305.,  44.,  59., 548., 226.,  60.,  46.])\n",
        "weights = 1/pops\n",
        "weights = weights/np.mean(weights)\n",
        "weights = torch.tensor(weights, dtype=torch.float32).to('cuda')\n",
        "balanced_criterion = nn.CrossEntropyLoss(weight=weights)"
      ],
      "metadata": {
        "id": "TrTh54MdD8jK"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Ob__pEhBnOz",
        "outputId": "8dd82496-77d6-4430-fa07-2833d8c2b304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/20 [06:43<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0:\n",
            "train loss: 0.9219, valid loss: 0.9352, delta loss: -0.0133,\n",
            "acc train: 0.1879, acc valid: 0.3329, standard train loss : 2.6912, standard valid loss : 2.5488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 1/20 [14:19<2:24:36, 456.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1:\n",
            "train loss: 0.7971, valid loss: 0.831, delta loss: -0.0339,\n",
            "acc train: 0.3551, acc valid: 0.3397, standard train loss : 2.3632, standard valid loss : 2.3048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 2/20 [21:05<2:07:57, 426.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2:\n",
            "train loss: 0.6673, valid loss: 0.7513, delta loss: -0.084,\n",
            "acc train: 0.4709, acc valid: 0.3874, standard train loss : 2.0398, standard valid loss : 2.1105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 3/20 [27:51<1:58:08, 416.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3:\n",
            "train loss: 0.5567, valid loss: 0.6631, delta loss: -0.1064,\n",
            "acc train: 0.545, acc valid: 0.513, standard train loss : 1.7785, standard valid loss : 1.8127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 4/20 [34:36<1:49:59, 412.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4:\n",
            "train loss: 0.4518, valid loss: 0.6465, delta loss: -0.1947,\n",
            "acc train: 0.6035, acc valid: 0.5075, standard train loss : 1.5383, standard valid loss : 1.6611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 5/20 [41:22<1:42:29, 409.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5:\n",
            "train loss: 0.3584, valid loss: 0.5797, delta loss: -0.2213,\n",
            "acc train: 0.6487, acc valid: 0.5553, standard train loss : 1.3161, standard valid loss : 1.5275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 6/20 [48:07<1:35:18, 408.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6:\n",
            "train loss: 0.2745, valid loss: 0.5418, delta loss: -0.2673,\n",
            "acc train: 0.693, acc valid: 0.5839, standard train loss : 1.1144, standard valid loss : 1.3751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 7/20 [54:50<1:28:16, 407.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7:\n",
            "train loss: 0.2064, valid loss: 0.549, delta loss: -0.3426,\n",
            "acc train: 0.7505, acc valid: 0.603, standard train loss : 0.9087, standard valid loss : 1.3127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 8/20 [1:01:34<1:21:11, 405.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8:\n",
            "train loss: 0.1538, valid loss: 0.557, delta loss: -0.4032,\n",
            "acc train: 0.7967, acc valid: 0.6357, standard train loss : 0.7324, standard valid loss : 1.2046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 9/20 [1:08:20<1:14:21, 405.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9:\n",
            "train loss: 0.1124, valid loss: 0.5604, delta loss: -0.4481,\n",
            "acc train: 0.8349, acc valid: 0.6357, standard train loss : 0.5845, standard valid loss : 1.1882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 10/20 [1:15:05<1:07:35, 405.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10:\n",
            "train loss: 0.0805, valid loss: 0.6156, delta loss: -0.5352,\n",
            "acc train: 0.8763, acc valid: 0.6303, standard train loss : 0.4469, standard valid loss : 1.1849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 11/20 [1:21:51<1:00:49, 405.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11:\n",
            "train loss: 0.0571, valid loss: 0.6708, delta loss: -0.6138,\n",
            "acc train: 0.9093, acc valid: 0.6685, standard train loss : 0.3372, standard valid loss : 1.1896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 12/20 [1:28:37<54:04, 405.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12:\n",
            "train loss: 0.0396, valid loss: 0.6967, delta loss: -0.6571,\n",
            "acc train: 0.9358, acc valid: 0.6644, standard train loss : 0.2459, standard valid loss : 1.1949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 13/20 [1:31:24<49:13, 421.89s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-30359f19c625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mstandard_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), \n",
        "    # lr=1,\n",
        "    lr=3e-5,\n",
        "    # weight_decay=0.0001\n",
        "    )\n",
        "epochs = 20\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
        "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-4) * ((epoch - epochs)/ epochs))\n",
        "criterion = balanced_criterion\n",
        "standard_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "shuffle_train = np.arange(len(sequences_train))\n",
        "shuffle_valid = np.arange(len(sequences_valid))\n",
        "\n",
        "batchSize = 4\n",
        "\n",
        "patience = 1_000\n",
        "patience_count = 0\n",
        "\n",
        "previous_epoch_loss_valid = 300\n",
        "best_epoch_standard_loss_valid = 300\n",
        "\n",
        "res = list()\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    epoch_loss, epoch_loss_valid, epoch_accuracy, epoch_accuracy_valid = [], [], [], []\n",
        "    epoch_standard_loss, epoch_standard_loss_valid = [], []\n",
        "\n",
        "    np.random.shuffle(shuffle_train)\n",
        "    np.random.shuffle(shuffle_valid)\n",
        "\n",
        "    # Train\n",
        "    batchSize=2\n",
        "    model.train()\n",
        "    for i in range(0, len(sequences_train), batchSize):\n",
        "\n",
        "        incr_i = min(i+batchSize, len(sequences_train))\n",
        "        indices = shuffle_train[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_train[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "        \n",
        "        sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_train[indices]]).to(device)\n",
        "\n",
        "        attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_train[indices]]).to(device)\n",
        "\n",
        "        # features_ = np.array(features_train)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "        # cum_nodes = adj_shapes_train[indices]\n",
        "        # idx_batch = np.repeat(np.arange(\n",
        "        #     batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
        "        #     ), cum_nodes)\n",
        "        # idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        # adj_ = adj_train[indices]\n",
        "        # adj_ = sp.block_diag(adj_)\n",
        "        # adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(sequences_, attention_mask_)\n",
        "\n",
        "        # Backward\n",
        "        loss = criterion(output, targets_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Compute metrics\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        standard_loss = standard_criterion(output, targets_)\n",
        "        epoch_standard_loss.append(standard_loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy.append(accuracy)\n",
        "\n",
        "    # Validation\n",
        "    batchSize=4\n",
        "    model.eval()\n",
        "    for i in range(0, len(sequences_valid), int(batchSize/4)):\n",
        "\n",
        "        incr_i = min(i+int(batchSize/4), len(sequences_valid))\n",
        "        indices = shuffle_valid[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_valid[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "        \n",
        "        sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_valid[indices]]).to(device)\n",
        "\n",
        "        attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_valid[indices]]).to(device)\n",
        "\n",
        "        # features_ = np.array(features_valid)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "        # cum_nodes = adj_shapes_valid[indices]\n",
        "        # idx_batch = np.repeat(np.arange(\n",
        "        #     int(batchSize/4) if incr_i%int(batchSize/4)==0 else incr_i%int(batchSize/4)\n",
        "        #     ), cum_nodes)\n",
        "        # idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        # adj_ = adj_valid[indices]\n",
        "        # adj_ = sp.block_diag(adj_)\n",
        "        # adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        output = model(sequences_, attention_mask_)\n",
        "\n",
        "        # Compute metrics\n",
        "        loss = criterion(output, targets_)\n",
        "        epoch_loss_valid.append(loss.item())\n",
        "\n",
        "        standard_loss = standard_criterion(output, targets_)\n",
        "        epoch_standard_loss_valid.append(standard_loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy_valid.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "    epoch_loss = np.array(epoch_loss).mean()\n",
        "    epoch_standard_loss = np.array(epoch_standard_loss).mean()\n",
        "    epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
        "    epoch_standard_loss_valid = np.array(epoch_standard_loss_valid).mean()\n",
        "\n",
        "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "    epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
        "\n",
        "    tqdm.write(\n",
        "        '\\n'\n",
        "        f'Epoch {epoch}:\\ntrain loss: {round(epoch_loss, 4)}, '\n",
        "        f'valid loss: {round(epoch_loss_valid, 4)}, delta loss: {round(epoch_loss-epoch_loss_valid, 4)},\\nacc train: {round(epoch_accuracy, 4)}, '\n",
        "        f'acc valid: {round(epoch_accuracy_valid, 4)}, '\n",
        "        f'standard train loss : {round(epoch_standard_loss, 4)}, '\n",
        "        f'standard valid loss : {round(epoch_standard_loss_valid, 4)}'\n",
        "        )\n",
        "    \n",
        "    # Save best model\n",
        "    if epoch_standard_loss_valid < best_epoch_standard_loss_valid:\n",
        "      torch.save({\"state\": model.state_dict(),}, \"last_model_checkpoint.pt\")\n",
        "      best_epoch__standard_loss_valid = epoch_standard_loss_valid\n",
        "\n",
        "    # Early stopping :\n",
        "    if previous_epoch_loss_valid < epoch_standard_loss_valid:\n",
        "        patience_count +=1\n",
        "        if patience_count == patience:\n",
        "          print(f'Early stopping end of epoch {epoch}')\n",
        "          break\n",
        "    else :\n",
        "        previous_epoch_loss_valid = epoch_loss_valid\n",
        "        patience_count = 0\n",
        "\n",
        "        # Save last best results\n",
        "        res = list()\n",
        "        shuffle_test = np.arange(len(sequences_test))\n",
        "\n",
        "        for i in range(0, len(sequences_test), int(batchSize/2)):\n",
        "            model.eval()\n",
        "\n",
        "            incr_i = min(i+int(batchSize/2), len(sequences_test))\n",
        "            sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_test[shuffle_test[i: incr_i]]])\n",
        "            attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_test[shuffle_test[i: incr_i]]])\n",
        "\n",
        "            output = model(sequences_, attention_mask_)\n",
        "            \n",
        "            res.append(output.to('cpu').detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), \n",
        "    # lr=1,\n",
        "    lr=3e-5,\n",
        "    # weight_decay=0.0001\n",
        "    )\n",
        "epochs = 20\n",
        "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
        "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-4) * ((epoch - epochs)/ epochs))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "shuffle_train = np.arange(len(sequences_train))\n",
        "shuffle_valid = np.arange(len(sequences_valid))\n",
        "\n",
        "batchSize = 4\n",
        "\n",
        "patience = 2\n",
        "patience_count = 0\n",
        "\n",
        "previous_epoch_loss_valid = 300\n",
        "\n",
        "res = list()\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    epoch_loss, epoch_loss_valid, epoch_accuracy, epoch_accuracy_valid = [], [], [], []\n",
        "\n",
        "    np.random.shuffle(shuffle_train)\n",
        "    np.random.shuffle(shuffle_valid)\n",
        "\n",
        "\n",
        "    # Train\n",
        "    batchSize=2\n",
        "    model.train()\n",
        "    for i in range(0, len(sequences_train), batchSize):\n",
        "\n",
        "        incr_i = min(i+batchSize, len(sequences_train))\n",
        "        indices = shuffle_train[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_train[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "        \n",
        "        sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_train[indices]]).to(device)\n",
        "\n",
        "        attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_train[indices]]).to(device)\n",
        "\n",
        "        # features_ = np.array(features_train)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "        # cum_nodes = adj_shapes_train[indices]\n",
        "        # idx_batch = np.repeat(np.arange(\n",
        "        #     batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
        "        #     ), cum_nodes)\n",
        "        # idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        # adj_ = adj_train[indices]\n",
        "        # adj_ = sp.block_diag(adj_)\n",
        "        # adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(sequences_, attention_mask_)\n",
        "        \n",
        "        # del features_\n",
        "        # del adj_\n",
        "        # del idx_batch\n",
        "        del sequences_\n",
        "        del attention_mask_\n",
        "\n",
        "        # Backward\n",
        "        loss = criterion(output, targets_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute metrics\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # Validation\n",
        "    batchSize=4\n",
        "    model.eval()\n",
        "    for i in range(0, len(sequences_valid), int(batchSize/4)):\n",
        "\n",
        "        incr_i = min(i+int(batchSize/4), len(sequences_valid))\n",
        "        indices = shuffle_valid[i: incr_i]\n",
        "\n",
        "        targets_ = torch.tensor(y_valid[indices])\n",
        "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "        \n",
        "        sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_valid[indices]]).to(device)\n",
        "\n",
        "        attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_valid[indices]]).to(device)\n",
        "\n",
        "        # features_ = np.array(features_valid)[indices]\n",
        "        # features_ = np.vstack(features_)\n",
        "        # features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "        # cum_nodes = adj_shapes_valid[indices]\n",
        "        # idx_batch = np.repeat(np.arange(\n",
        "        #     int(batchSize/4) if incr_i%int(batchSize/4)==0 else incr_i%int(batchSize/4)\n",
        "        #     ), cum_nodes)\n",
        "        # idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "        # adj_ = adj_valid[indices]\n",
        "        # adj_ = sp.block_diag(adj_)\n",
        "        # adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "        output = model(sequences_, attention_mask_)\n",
        "\n",
        "        # del features_\n",
        "        # del adj_\n",
        "        # del idx_batch\n",
        "        del sequences_\n",
        "        del attention_mask_\n",
        "\n",
        "        # Compute metrics\n",
        "        loss = criterion(output, targets_)\n",
        "        epoch_loss_valid.append(loss.item())\n",
        "        \n",
        "        targets_ = targets_.to('cpu').detach().numpy()\n",
        "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "        epoch_accuracy_valid.append(accuracy)\n",
        "\n",
        "        del output\n",
        "\n",
        "    # scheduler.step()\n",
        "\n",
        "    epoch_loss = np.array(epoch_loss).mean()\n",
        "    epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
        "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "    epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
        "\n",
        "    tqdm.write(\n",
        "        '\\n'\n",
        "        f'Epoch {epoch}:\\ntrain loss: {round(epoch_loss, 4)}, '\n",
        "        f'valid loss: {round(epoch_loss_valid, 4)}, delta loss: {round(epoch_loss-epoch_loss_valid, 4)},\\nacc train: {round(epoch_accuracy, 4)}, '\n",
        "        f'acc valid: {round(epoch_accuracy_valid, 4)}'\n",
        "        )\n",
        "    \n",
        "    # Early stopping :\n",
        "    if previous_epoch_loss_valid < epoch_loss_valid:\n",
        "        if patience_count == 0:\n",
        "          torch.save({\"state\": model.state_dict(),}, \"last_model_checkpoint.pt\")\n",
        "        patience_count +=1\n",
        "        if patience_count == patience:\n",
        "          print(f'Early stopping end of epoch {epoch}')\n",
        "          break\n",
        "\n",
        "    else :\n",
        "        previous_epoch_loss_valid = epoch_loss_valid\n",
        "        patience_count = 0\n",
        "\n",
        "        # Save last best results\n",
        "        del res\n",
        "        res = list()\n",
        "        shuffle_test = np.arange(len(sequences_test))\n",
        "\n",
        "        for i in range(0, len(sequences_test), int(batchSize/2)):\n",
        "            model.eval()\n",
        "\n",
        "            incr_i = min(i+int(batchSize/2), len(sequences_test))\n",
        "            sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_test[shuffle_test[i: incr_i]]])\n",
        "            attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_test[shuffle_test[i: incr_i]]])\n",
        "\n",
        "            output = model(sequences_, attention_mask_)\n",
        "\n",
        "            del sequences_\n",
        "            del attention_mask_\n",
        "            \n",
        "            res.append(output.to('cpu').detach().numpy())\n",
        "\n",
        "            del output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "5aD3r1z8DFmI",
        "outputId": "63f25ad0-8d6b-4f7b-8d86-4f024a431a2a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/20 [03:51<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0:\n",
            "train loss: 0.2046, valid loss: 1.2321, delta loss: -1.0275,\n",
            "acc train: 0.9497, acc valid: 0.6671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 1/20 [08:15<1:23:09, 262.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1:\n",
            "train loss: 0.1003, valid loss: 1.2744, delta loss: -1.1741,\n",
            "acc train: 0.9815, acc valid: 0.6685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 2/20 [08:35<1:17:20, 257.80s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-1817adef4e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtargets_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9dlnS-EsCQqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJW8taWK2kGf"
      },
      "outputs": [],
      "source": [
        "bs = 4 en changeant les poids des deux dernières couches, avec 0.2 en std\n",
        "\n",
        "train loss: 0.8832, valid loss: 1.2811, delta loss: -0.3979,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21erU3jT7kOB"
      },
      "outputs": [],
      "source": [
        "bs = 4 en changeant les poids des deux dernières couches, avec 0.1 en std\n",
        "\n",
        "train loss: 0.8925, valid loss: 1.2393, delta loss: -0.3468,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqXN3F79y4oY"
      },
      "outputs": [],
      "source": [
        "bs = 4 sans changer les poids de la dernière couche\n",
        "\n",
        "train loss: 0.79, valid loss: 1.2503, delta loss: -0.4603,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ESbR2dhz5dbF"
      },
      "outputs": [],
      "source": [
        "sub = list(map(lambda x: np.array(nn.Softmax(dim=1)(torch.tensor(x))), res))\n",
        "\n",
        "y_pred = np.concatenate(sub, axis=0)\n",
        "y_pred.shape\n",
        "\n",
        "proteins_test = list()\n",
        "with open('data/graph_labels.txt', 'r') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        t = line.split(',')\n",
        "        if len(t[1][:-1]) == 0:\n",
        "            proteins_test.append(t[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "uDAIvAQvA6rl"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('balancedseq3e5_bs4.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    lst = list()\n",
        "    for i in range(18):\n",
        "        lst.append('class'+str(i))\n",
        "    lst.insert(0, \"name\")\n",
        "    writer.writerow(lst)\n",
        "    for i, protein in enumerate(proteins_test):\n",
        "        lst = y_pred[i,:].tolist()\n",
        "        lst.insert(0, protein)\n",
        "        writer.writerow(lst)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PqcnRRZfhyC",
        "outputId": "5cb2b58f-2243-480b-a309-c65b323f9ef3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 8epochs_samedi_uniform_bs2_lr3e5.csv\n",
            "\u001b[0m\u001b[01;34m'Appartement rue Friant '\u001b[0m/\n",
            " Astrologie.WMA\n",
            " \u001b[01;34mAuto-Entrepreneur\u001b[0m/\n",
            " balancedseq3e5_bs4.csv\n",
            " \u001b[01;34mCaf\u001b[0m/\n",
            " \u001b[01;34mCEA\u001b[0m/\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            "\u001b[01;34m'Cour des comptes'\u001b[0m/\n",
            " \u001b[01;34mdata\u001b[0m/\n",
            " \u001b[01;34mENSAE\u001b[0m/\n",
            "\u001b[01;34m'Fauriel '\u001b[0m/\n",
            "'Filiatre Hypnotisme et magnétisme 1905.pdf'\n",
            " \u001b[01;34mHumanitics\u001b[0m/\n",
            "'Journée Défense et Citoyenneté .pdf'\n",
            " last_model_checkpoint.pt\n",
            "'Lettre de motivation.gdoc'\n",
            " \u001b[01;34mMedical\u001b[0m/\n",
            " passeport.pdf\n",
            "'Permis recto.jpg'\n",
            "'Permis verso.jpg'\n",
            " Samuel.gdoc\n",
            "'Thibaut VALOUR Mathématiques de l'\\''aléatoire Lettre de recommandation (1).gdoc'\n",
            "\"Thibaut VALOUR Mathématiques de l'aléatoire Lettre de recommandation.gdoc\"\n",
            "\"Thibaut VALOUR Mathématiques de l'aléatoire Lettre de recommandation.pdf\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdXPgLJqAqeG"
      },
      "outputs": [],
      "source": [
        "Epoch 0:\n",
        "train loss: 1.8907, valid loss: 1.6979,\n",
        "acc train: 0.4579acc valid: 0.485\n",
        "Epoch 1:\n",
        "train loss: 1.3323, valid loss: 1.4126,\n",
        "acc train: 0.622acc valid: 0.5913\n",
        "Epoch 2:\n",
        "train loss: 1.0555, valid loss: 1.2206,\n",
        "acc train: 0.7087acc valid: 0.6335\n",
        "Epoch 3:\n",
        "train loss: 0.8302, valid loss: 1.1845,\n",
        "acc train: 0.7745acc valid: 0.6662\n",
        "Epoch 4:\n",
        "train loss: 0.649, valid loss: 1.2185,\n",
        "acc train: 0.8267acc valid: 0.6499\n",
        "Epoch 5:\n",
        "train loss: 0.4991, valid loss: 1.1734,\n",
        "acc train: 0.8745acc valid: 0.6703\n",
        "Epoch 6:\n",
        "train loss: 0.3834, valid loss: 1.1397,\n",
        "acc train: 0.909acc valid: 0.688\n",
        "Epoch 7:\n",
        "train loss: 0.2994, valid loss: 1.1628,\n",
        "acc train: 0.9351acc valid: 0.6839\n",
        "Epoch 8:\n",
        "train loss: 0.238, valid loss: 1.186,\n",
        "acc train: 0.9553acc valid: 0.6826\n",
        "Early stopping end of epoch 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqC8fhG6AlkX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vX27zXn_V69"
      },
      "outputs": [],
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgyKhmKYqP-W"
      },
      "outputs": [],
      "source": [
        "  5%|▌         | 1/20 [03:28<1:05:59, 208.38s/it]\n",
        "Epoch 0:\n",
        "train loss: 1.9055, valid loss: 1.6867,\n",
        "acc train: 0.4484acc valid: 0.4946\n",
        " 10%|█         | 2/20 [06:56<1:02:28, 208.27s/it]\n",
        "Epoch 1:\n",
        "train loss: 1.3533, valid loss: 1.4342,\n",
        "acc train: 0.6246acc valid: 0.5681\n",
        " 15%|█▌        | 3/20 [10:24<59:01, 208.30s/it]  \n",
        "Epoch 2:\n",
        "train loss: 1.0844, valid loss: 1.3051,\n",
        "acc train: 0.6972acc valid: 0.6131\n",
        " 20%|██        | 4/20 [13:52<55:30, 208.18s/it]\n",
        "Epoch 3:\n",
        "train loss: 0.8398, valid loss: 1.248,\n",
        "acc train: 0.7732acc valid: 0.6349\n",
        " 25%|██▌       | 5/20 [17:21<52:02, 208.15s/it]\n",
        "Epoch 4:\n",
        "train loss: 0.6354, valid loss: 1.2457,\n",
        "acc train: 0.8288acc valid: 0.6471\n",
        " 30%|███       | 6/20 [20:49<48:33, 208.13s/it]\n",
        "Epoch 5:\n",
        "train loss: 0.4625, valid loss: 1.1694,\n",
        "acc train: 0.8825acc valid: 0.6662\n",
        " 35%|███▌      | 7/20 [24:17<45:05, 208.15s/it]\n",
        "Epoch 6:\n",
        "train loss: 0.3168, valid loss: 1.1416,\n",
        "acc train: 0.9216acc valid: 0.6771\n",
        " 35%|███▌      | 7/20 [27:45<45:05, 208.15s/it]\n",
        "Epoch 7:\n",
        "train loss: 0.2154, valid loss: 1.1496,\n",
        "acc train: 0.9583acc valid: 0.6798\n",
        " 40%|████      | 8/20 [31:15<46:52, 234.40s/it]\n",
        "Epoch 8:\n",
        "train loss: 0.1402, valid loss: 1.204,\n",
        "acc train: 0.9791acc valid: 0.6812\n",
        "Early stopping end of epoch 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrGYY1dSTQ4G",
        "outputId": "8e7c03a7-ed11-46b5-a1ed-e2634407c3c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = torch.load(\"../gdrive/MyDrive/data/last_model_checkpoint.pt\")\n",
        "model.load_state_dict(checkpoint['state'])\n",
        "model.eval()\n",
        "()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY6Z3fWoZyk9"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl43TbMIaId3"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "os.environ [\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:516\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64TGvGnRa3P9"
      },
      "outputs": [],
      "source": [
        "model = model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "ICI4CdCiUAYL",
        "outputId": "3f72af41-57e2-4a1d-cd26-fdfd77d1e7ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1223 [00:05<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2451dbdaee4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mfeatures_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ec1fe8b1ee6f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         output = self.bert(\n\u001b[0m\u001b[1;32m     13\u001b[0m           \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1021\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 426\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.80 GiB (GPU 0; 39.59 GiB total capacity; 32.94 GiB already allocated; 2.76 GiB free; 35.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "batchSize = 2\n",
        "res = list()\n",
        "\n",
        "indices = np.arange(len(sequences_test))\n",
        "\n",
        "for i in tqdm(range(0, len(sequences_test), int(batchSize/2))):\n",
        "    model.eval()\n",
        "\n",
        "    incr_i = min(i+int(batchSize/2), len(sequences_test))\n",
        "    # indices = shuffle_test[i: incr_i]\n",
        "\n",
        "    # targets_ = torch.tensor(y_valid[indices])\n",
        "    # targets_ = F.one_hot(targets_, 18).float().to(device)\n",
        "    \n",
        "    sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences_test[indices]])\n",
        "\n",
        "    attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences_test[indices]])\n",
        "\n",
        "    # features_ = np.array(features_test)[indices]\n",
        "    # features_ = np.vstack(features_)\n",
        "    # features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "    # cum_nodes = adj_shapes_test[indices]\n",
        "    # idx_batch = np.repeat(np.arange(\n",
        "    #     int(batchSize/4) if incr_i%int(batchSize/4)==0 else incr_i%int(batchSize/4)\n",
        "    #     ), cum_nodes)\n",
        "    # idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "    \n",
        "    # adj_ = adj_test[indices]\n",
        "    # adj_ = sp.block_diag(adj_)\n",
        "    # adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "    output = model(sequences_, attention_mask_)\n",
        "\n",
        "    del features_\n",
        "    del adj_\n",
        "    del idx_batch\n",
        "    del sequences_\n",
        "    del attention_mask_\n",
        "    \n",
        "    res.append(output)\n",
        "\n",
        "    del output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c28YLr3W0jB"
      },
      "outputs": [],
      "source": [
        "arg1 = np.arange(\n",
        "  int(batchSize/4) if incr_i%int(batchSize/4)==0 else incr_i%int(batchSize/4)\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgEyAa3TW2em",
        "outputId": "d124db05-9b2b-432b-94bb-a849ab092225"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1223,)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cum_nodes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv4ikvmnWwsE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3lgq9iDT3aF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JHdVz8RDVZT"
      },
      "outputs": [],
      "source": [
        "  5%|▌         | 1/20 [03:20<1:03:26, 200.36s/it]\n",
        "Epoch 0:\n",
        "train loss: 1.8427, valid loss: 1.6749,\n",
        "acc train: 0.4519acc valid: 0.4986\n",
        " 10%|█         | 2/20 [06:40<1:00:03, 200.17s/it]\n",
        "Epoch 1:\n",
        "train loss: 1.3959, valid loss: 1.4417,\n",
        "acc train: 0.5976acc valid: 0.5938\n",
        " 10%|█         | 2/20 [10:00<1:00:03, 200.17s/it]\n",
        "Epoch 2:\n",
        "train loss: 1.1997, valid loss: 1.4535,\n",
        "acc train: 0.6586acc valid: 0.5978\n",
        " 15%|█▌        | 3/20 [13:21<1:15:40, 267.10s/it]\n",
        "Epoch 3:\n",
        "train loss: 1.046, valid loss: 1.5351,\n",
        "acc train: 0.7006acc valid: 0.5557\n",
        "Early stopping end of epoch 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-brUXnR_m5SK",
        "outputId": "d4ca5600-c65d-41a2-835e-eabebb4bbc4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " 10%|█         | 1/10 [03:22<30:19, 202.12s/it]\n",
        "Epoch 0, train loss: 1.8413, valid loss: 2.0762\n",
        ", acc train: 0.4657acc train: 0.3791\n",
        " 20%|██        | 2/10 [06:44<26:56, 202.10s/it]\n",
        "Epoch 1, train loss: 1.6625, valid loss: 1.7745\n",
        ", acc train: 0.5146acc train: 0.4742\n",
        " 20%|██        | 2/10 [10:06<40:24, 303.07s/it]\n",
        "Epoch 2, train loss: 1.5559, valid loss: 1.8608\n",
        ", acc train: 0.553acc train: 0.5068\n",
        "Early stopping end of epoch 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8StguImHwSOB"
      },
      "outputs": [],
      "source": [
        "10%|█         | 1/10 [03:22<30:19, 202.14s/it]\n",
        "Epoch 0:\n",
        "train loss: 2.9345, valid loss: 2.6558,\n",
        "acc train: 0.2496acc valid: 0.2908\n",
        " 20%|██        | 2/10 [06:44<26:56, 202.11s/it]\n",
        "Epoch 1:\n",
        "train loss: 2.2657, valid loss: 2.4044,\n",
        "acc train: 0.3677acc valid: 0.3152\n",
        " 30%|███       | 3/10 [10:06<23:34, 202.09s/it]\n",
        "Epoch 2:\n",
        "train loss: 1.9986, valid loss: 2.3356,\n",
        "acc train: 0.4311acc valid: 0.3356\n",
        " 40%|████      | 4/10 [13:28<20:12, 202.01s/it]\n",
        "Epoch 3:\n",
        "train loss: 1.8384, valid loss: 1.9591,\n",
        "acc train: 0.4693acc valid: 0.4389\n",
        " 40%|████      | 4/10 [16:50<25:15, 252.53s/it]\n",
        "Epoch 4:\n",
        "train loss: 1.6969, valid loss: 2.1203,\n",
        "acc train: 0.5152acc valid: 0.4524\n",
        "Early stopping end of epoch 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nnLO7dp98O4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6E0c9X_Ar2b"
      },
      "outputs": [],
      "source": [
        "# for batch_num, e in enumerate(get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
        "#                 tokenizer=tokenizer, max_len=600, batch_size=8, shuffle=True, version=version, drop_last=True)):\n",
        "        \n",
        "#         batch_indices = e['indexs']\n",
        "        \n",
        "\n",
        "#         target = F.one_hot(e['target'], 18).float().to(device)\n",
        "\n",
        "#         #Compute sequence forward\n",
        "#         input = e['input_ids'].to(device)\n",
        "#         src_mask = e['attention_mask'].to(device)\n",
        "\n",
        "#         # Compute graph forward\n",
        "#         features_ = np.array(features)[batch_indices]\n",
        "#         features_ = np.vstack(features_)\n",
        "#         features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "#         adj_ = adj[batch_indices]\n",
        "#         adj_ = sp.block_diag(adj_)\n",
        "#         adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "#         cum_nodes = adj_shapes[batch_indices]\n",
        "#         idx_batch = np.repeat(np.arange(\n",
        "#             len(batch_indices)\n",
        "#             ), cum_nodes)\n",
        "#         idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "        \n",
        "#         output = model(features_, adj_, idx_batch, input, src_mask)\n",
        "\n",
        "#         # Backward\n",
        "#         loss = criterion(output, target)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Compute metrics\n",
        "#         epoch_loss.append(loss.item())\n",
        "        \n",
        "#         target = target.to('cpu').detach().numpy()\n",
        "#         output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "#         accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "#         epoch_accuracy.append(accuracy)\n",
        "\n",
        "#     for batch_num, e in enumerate(get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
        "#                 tokenizer=tokenizer, max_len=600, batch_size=8, shuffle=False, version=\"valid\", drop_last=False)):\n",
        "      \n",
        "#         batch_indices = e['indexs']\n",
        "#         target = F.one_hot(e['target'], 18).float().to(device)\n",
        "\n",
        "#         #Compute sequence forward\n",
        "#         input = e['input_ids'].to(device)\n",
        "#         src_mask = e['attention_mask'].to(device)\n",
        "\n",
        "#         # Compute graph forward\n",
        "#         features_ = np.array(features_valid)[batch_indices]\n",
        "#         features_ = np.vstack(features_)\n",
        "#         features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "#         adj_ = adj_valid[batch_indices]\n",
        "#         adj_ = sp.block_diag(adj_)\n",
        "#         adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "#         cum_nodes = adj_shapes_valid[batch_indices]\n",
        "#         idx_batch = np.repeat(np.arange(\n",
        "#             len(batch_indices)\n",
        "#             ), cum_nodes)\n",
        "#         idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "        \n",
        "#         output = model(features_, adj_, idx_batch, input, src_mask)\n",
        "\n",
        "#         # Compute metrics\n",
        "#         epoch_loss_valid.append(loss.item())\n",
        "        \n",
        "#         target = target.to('cpu').detach().numpy()\n",
        "#         output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
        "\n",
        "#         accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
        "#         epoch_accuracy_valid.append(accuracy)\n",
        "\n",
        "\n",
        "#     scheduler.step()\n",
        "\n",
        "#     epoch_loss = np.array(epoch_loss).mean()\n",
        "#     epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
        "#     epoch_accuracy = np.array(epoch_accuracy).mean()\n",
        "#     epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
        "\n",
        "#     tqdm.write(\n",
        "#         '\\n'\n",
        "#         f'Epoch {epoch}, train loss: {round(epoch_loss, 4)}, '\n",
        "#         f'valid loss: {round(epoch_loss_valid, 4)}\\n, acc train: {round(epoch_accuracy, 4)}'\n",
        "#         f'acc train: {round(epoch_accuracy_valid, 4)}'\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NJ6ZPzwBnOz"
      },
      "outputs": [],
      "source": [
        "torch.save({\"state\": model.state_dict(),}, \"1901__5epochs.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv2_W25bBnOz"
      },
      "outputs": [],
      "source": [
        "version = \"valid\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co0DoZjFQVhq"
      },
      "outputs": [],
      "source": [
        "ref = list()\n",
        "with open('data/graph_labels.txt', 'r') as f:\n",
        "    for i,line in enumerate(f):\n",
        "        t = line.split(',')\n",
        "        if len(t[1][:-1]) == 0:\n",
        "            ref.append(True)\n",
        "        else:\n",
        "            ref.append(False)\n",
        "\n",
        "ref = np.array(ref)\n",
        "ref = np.logical_not(ref)\n",
        "ref.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kxESA96QZGD"
      },
      "outputs": [],
      "source": [
        "is_kept = []\n",
        "with open('data/graph_labels.txt', \"r\") as f1:\n",
        "    for line in f1:\n",
        "        s1, s2 = line.strip().split(',')\n",
        "        if len(s2.strip())>0:\n",
        "            is_kept.append(version != \"valid\")\n",
        "        else :\n",
        "            is_kept.append(version == \"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMZNHasvQame"
      },
      "outputs": [],
      "source": [
        "adj, adj_weight, features, edge_features, Flist = load_data()\n",
        "del Flist\n",
        "del edge_features\n",
        "\n",
        "adj = [normalize_adjacency(A, W) for A, W in zip(adj, adj_weight)]\n",
        "adj_shapes = np.array([at.shape[0] for at in adj])\n",
        "adj = [adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))]\n",
        "\n",
        "adj = np.array(adj)[np.array(is_kept)]\n",
        "features = np.array(features, dtype=object)[np.array(is_kept)]\n",
        "adj_shapes = adj_shapes[np.array(is_kept)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPD6aqzHQgIx"
      },
      "outputs": [],
      "source": [
        "len(adj), len(features), len(adj_shapes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67gre-U1Qg62"
      },
      "outputs": [],
      "source": [
        "data_loader = get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
        "                tokenizer=tokenizer, max_len=600, batch_size=8, shuffle=False, version=version, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qzws1nfhBSV"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model.training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QdCSLFpVQi6T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "1990265c-9a09-4167-987c-6fe3d25d56df"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-ed4edd3a979b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'indexs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
          ]
        }
      ],
      "source": [
        "sub = []\n",
        "\n",
        "for batch_num, e in tqdm(enumerate(data_loader)):\n",
        "        \n",
        "    batch_indices = e['indexs']\n",
        "\n",
        "    #Compute sequence forward\n",
        "    input = e['input_ids'].to(device)\n",
        "    src_mask = e['attention_mask'].to(device)\n",
        "\n",
        "    # Compute graph forward\n",
        "    features_ = np.array(features)[batch_indices]\n",
        "    features_ = np.vstack(features_)\n",
        "    features_ = torch.FloatTensor(features_).to(device)\n",
        "\n",
        "    adj_ = adj[batch_indices]\n",
        "    adj_ = sp.block_diag(adj_)\n",
        "    adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
        "\n",
        "    cum_nodes = adj_shapes[batch_indices]\n",
        "    idx_batch = np.repeat(np.arange(\n",
        "        len(batch_indices)\n",
        "        ), cum_nodes)\n",
        "    idx_batch = torch.LongTensor(idx_batch).to(device)\n",
        "    \n",
        "    output = model(features_, adj_, idx_batch, input, src_mask)\n",
        "    \n",
        "    sub.append(output.to('cpu').detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsul_-4iBnOz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0kqBhyeTvtV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-sWYXpkUWN-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92P_td8SUAf7"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('5epochs_underfitting.csv', 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    lst = list()\n",
        "    for i in range(18):\n",
        "        lst.append('class'+str(i))\n",
        "    lst.insert(0, \"name\")\n",
        "    writer.writerow(lst)\n",
        "    for i, protein in enumerate(proteins_test):\n",
        "        lst = y_pred[i,:].tolist()\n",
        "        lst.insert(0, protein)\n",
        "        writer.writerow(lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQxUf-gPTosN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHUl3VoaBnOz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "k = 0\n",
        "\n",
        "résumé = []\n",
        "for k in range(18):\n",
        "    bol = tf.argmax(y_test, axis=1) == k\n",
        "    preds = model(X_test)[bol]\n",
        "    targets = y_test[bol]\n",
        "    accuracy_preds = (tf.argmax(preds, axis=1)==k).numpy().mean()\n",
        "    nll = log_loss(y_true=targets, y_pred=preds)\n",
        "    résumé.append((accuracy_preds, nll))\n",
        "\n",
        "acc = [tup[0] for tup in résumé]\n",
        "nll = [tup[1] for tup in résumé]\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, tight_layout=True, figsize=(14, 6))\n",
        "axs[0].bar(np.arange(len(acc)), acc)\n",
        "axs[0].set_title('Accuracy')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "axs[1].bar(np.arange(len(nll)), nll)\n",
        "axs[1].set_title('Negative Log Likelihood')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "axs[2].bar(np.arange(len(nll)), [440.,  50., 939.,  60., 112., 625., 202.,  74., 998.,  57.,  43.,305.,  44.,  59., 548., 226.,  60.,  46.])\n",
        "axs[2].set_title('Pops')\n",
        "plt.xticks(np.arange(len(acc)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz2189XIBnOz"
      },
      "outputs": [],
      "source": [
        "#GTN version brouillon\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "a = torch.ones(25, 300)\n",
        "b = torch.ones(22, 300)\n",
        "c = torch.ones(15, 300)\n",
        "pad_sequence([a, b, c]).size()\n",
        "\n",
        "\n",
        "\n",
        "from graph_transformer_pytorch import GraphTransformer\n",
        "\n",
        "model = GraphTransformer(\n",
        "    dim = 256,\n",
        "    depth = 6,\n",
        "    edge_dim = 512,             # optional - if left out, edge dimensions is assumed to be the same as the node dimensions above\n",
        "    with_feedforwards = True,   # whether to add a feedforward after each attention layer, suggested by literature to be needed\n",
        "    gated_residual = True,      # to use the gated residual to prevent over-smoothing\n",
        "    rel_pos_emb = True          # set to True if the nodes are ordered, default to False\n",
        ")\n",
        "\n",
        "nodes = torch.randn(1, 128, 256)\n",
        "edges = torch.randn(1, 128, 128, 512)\n",
        "mask = torch.ones(1, 128).bool()\n",
        "\n",
        "nodes, edges = model(nodes, edges, mask = mask)\n",
        "\n",
        "nodes.shape # (1, 128, 256) - project to R^3 for coordinates"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "u72E7pCrBnOw"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "altegrad_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "93201184ae5283544afdd58677953ee734bf67b299385e7b22daff49378f4f38"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5f0d9bfaab145eb9319b44ca5dd5b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5fb6925a94a4fc2a44166bffb05badd",
              "IPY_MODEL_59c0f7e8359348f39f35aa6f905844e0",
              "IPY_MODEL_b01ace8ca5794f769970c8b91ddefc18"
            ],
            "layout": "IPY_MODEL_b9171da9d1654ae5b651797bedae1551"
          }
        },
        "b5fb6925a94a4fc2a44166bffb05badd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ce38275890943e99e0a8cb08ef36538",
            "placeholder": "​",
            "style": "IPY_MODEL_50a6b3e896464727b935c343fab1ee18",
            "value": "Downloading: 100%"
          }
        },
        "59c0f7e8359348f39f35aa6f905844e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a027429cb73842e89a6f317c08c14a0a",
            "max": 80,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae64b29f95634c78be5b4c0c9402d929",
            "value": 80
          }
        },
        "b01ace8ca5794f769970c8b91ddefc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2b56add8c2e46edb7c3c367191843f6",
            "placeholder": "​",
            "style": "IPY_MODEL_f307e6b4de7245b68cf60d29a5000a3a",
            "value": " 80.0/80.0 [00:00&lt;00:00, 5.32kB/s]"
          }
        },
        "b9171da9d1654ae5b651797bedae1551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ce38275890943e99e0a8cb08ef36538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a6b3e896464727b935c343fab1ee18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a027429cb73842e89a6f317c08c14a0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae64b29f95634c78be5b4c0c9402d929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2b56add8c2e46edb7c3c367191843f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f307e6b4de7245b68cf60d29a5000a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c95b2020a3e48228a4be9bbbe41f88d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d10fec524697410c9ad8bf81c0b9e02a",
              "IPY_MODEL_81f63e2edc3e490dbaa2a4df2cf54f05",
              "IPY_MODEL_a887248da4da4e7ab1b291825de0641b"
            ],
            "layout": "IPY_MODEL_de11022356034ea5b59edd46a3205778"
          }
        },
        "d10fec524697410c9ad8bf81c0b9e02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d00fbf37f607422ca5b818f7671d2b3a",
            "placeholder": "​",
            "style": "IPY_MODEL_3c4a0e9f989643cfb9d226b600d1bf1a",
            "value": "Downloading: 100%"
          }
        },
        "81f63e2edc3e490dbaa2a4df2cf54f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_978b6af3c63543538f33693e031b0b6e",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e3172c1e7434e42a7629db1fc1dc5ac",
            "value": 112
          }
        },
        "a887248da4da4e7ab1b291825de0641b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bce78acdcb4c4edfa427e87ca5b142e6",
            "placeholder": "​",
            "style": "IPY_MODEL_4c8b61a4c9be4c50acd44088b66adef4",
            "value": " 112/112 [00:00&lt;00:00, 8.82kB/s]"
          }
        },
        "de11022356034ea5b59edd46a3205778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d00fbf37f607422ca5b818f7671d2b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c4a0e9f989643cfb9d226b600d1bf1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "978b6af3c63543538f33693e031b0b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e3172c1e7434e42a7629db1fc1dc5ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bce78acdcb4c4edfa427e87ca5b142e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8b61a4c9be4c50acd44088b66adef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f16d7844da74e90a657f4afb0673e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_babd46a6665b42e19a047c621e2ecd53",
              "IPY_MODEL_6b811e436583474cb46dedaab5756aa6",
              "IPY_MODEL_09ebdee1d767447e892a1abbe47dfdd0"
            ],
            "layout": "IPY_MODEL_f45ad67ced524b7ba52d80eb6bcde632"
          }
        },
        "babd46a6665b42e19a047c621e2ecd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_800d592ff7364e088a726ce8db18678c",
            "placeholder": "​",
            "style": "IPY_MODEL_ef8def30b5c84a959d5942090835ea72",
            "value": "Downloading: 100%"
          }
        },
        "6b811e436583474cb46dedaab5756aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a933ba3c2074bd38ce98127e5164839",
            "max": 86,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf3b2f3d33454437806b7d6ebefa3377",
            "value": 86
          }
        },
        "09ebdee1d767447e892a1abbe47dfdd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02dae53c12b14c73b8dda3d32cdca434",
            "placeholder": "​",
            "style": "IPY_MODEL_237be4600db94aec90358340807d53c9",
            "value": " 86.0/86.0 [00:00&lt;00:00, 6.75kB/s]"
          }
        },
        "f45ad67ced524b7ba52d80eb6bcde632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800d592ff7364e088a726ce8db18678c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef8def30b5c84a959d5942090835ea72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a933ba3c2074bd38ce98127e5164839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf3b2f3d33454437806b7d6ebefa3377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02dae53c12b14c73b8dda3d32cdca434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "237be4600db94aec90358340807d53c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a5751f01a1745a7b3c526d94f52d2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_578dfbffc17c479fa77030e2d4fd4fb6",
              "IPY_MODEL_09b37607b156462982e5a9ea91feca71",
              "IPY_MODEL_ce102f78889648c6b6ac8824caf7d08e"
            ],
            "layout": "IPY_MODEL_beb4aa6dc44745c0a8b442f34fa84d3b"
          }
        },
        "578dfbffc17c479fa77030e2d4fd4fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c71a6d8d84ae48818d21ed2fdafbc3c0",
            "placeholder": "​",
            "style": "IPY_MODEL_591b1c207c5f4958b71d653c320fc8ec",
            "value": "Downloading: 100%"
          }
        },
        "09b37607b156462982e5a9ea91feca71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c3338aaecdf4c569329b9792588a924",
            "max": 589,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f660e17d27c4ce8a5bd151ef911d10d",
            "value": 589
          }
        },
        "ce102f78889648c6b6ac8824caf7d08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70bb5a0a868d4dc5b78a84cbba6f6bcd",
            "placeholder": "​",
            "style": "IPY_MODEL_12e364868dc34945983141abac01bdb9",
            "value": " 589/589 [00:00&lt;00:00, 33.8kB/s]"
          }
        },
        "beb4aa6dc44745c0a8b442f34fa84d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c71a6d8d84ae48818d21ed2fdafbc3c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "591b1c207c5f4958b71d653c320fc8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c3338aaecdf4c569329b9792588a924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f660e17d27c4ce8a5bd151ef911d10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70bb5a0a868d4dc5b78a84cbba6f6bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12e364868dc34945983141abac01bdb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}