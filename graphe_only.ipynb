{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## COLAB AREA\n",
    "\n",
    "# !git clone https://github.com/AugustinCombes/zetaFold.git\n",
    "# %pip install transformers\n",
    "# # from google.colab import drive\n",
    "\n",
    "# # drive.mount('/content/gdrive/', force_remount=True)\n",
    "# # ../gdrive/MyDrive/data\n",
    "# %cd zetaFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import normalize_adjacency, sparse_mx_to_torch_sparse_tensor, load_data, submit_predictions, graph_train_valid_test\n",
    "from dataloader import get_loader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# from models.enhanced_graph import GTN\n",
    "from models.self_attention_graph import GTN\n",
    "from models.baseline_graph import GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 4888\n",
    "treshold_valid = 733\n",
    "\n",
    "ref = dict()\n",
    "with open('data/graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) != 0:\n",
    "            if len([_ for _ in ref.values() if _ == \"train\"]) < n_labels - treshold_valid:\n",
    "              ref[i] = \"train\"\n",
    "            else :\n",
    "              ref[i] = \"valid\"\n",
    "        else:\n",
    "            ref[i] = \"test\"\n",
    "\n",
    "ref_train = np.array([i for i in range(len(ref)) if ref[i]==\"train\"])\n",
    "ref_valid = np.array([i for i in range(len(ref)) if ref[i]==\"valid\"])\n",
    "ref_test = np.array([i for i in range(len(ref)) if ref[i]==\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# device = 'mps'\n",
    "# device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "\n",
    "y = []\n",
    "valid_id = []\n",
    "\n",
    "with open('data/graph_labels.txt', \"r\") as f1:\n",
    "  for line in f1:\n",
    "    s1, s2 = line.strip().split(',')\n",
    "    if len(s2.strip())>0:\n",
    "      y.append(int(s2))\n",
    "    else :\n",
    "      valid_id.append(s1)\n",
    "\n",
    "y = np.array(y)\n",
    "y_train, y_valid = y[:-treshold_valid], y[-treshold_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph features\n",
    "\n",
    "adj, adj_weight, features, edge_features, Flist = load_data()\n",
    "del Flist\n",
    "del edge_features\n",
    "\n",
    "(\n",
    "    adj_train, adj_valid, adj_test, \n",
    "    features_train, features_valid, features_test, \n",
    "    adj_shapes_train, adj_shapes_valid, adj_shapes_test\n",
    "                                                        ) = graph_train_valid_test(adj, adj_weight, features, ref_train, ref_valid, ref_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essai avec GTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTN(86, 64, 0.2, 18).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    # lr=1,\n",
    "    lr=1e-3,\n",
    "    # weight_decay=0.0001\n",
    "    )\n",
    "epochs = 50\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-4) * ((epoch - epochs)/ epochs))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "shuffle_train = np.arange(len(ref_train))\n",
    "shuffle_valid = np.arange(len(ref_valid))\n",
    "\n",
    "batchSize = 64\n",
    "\n",
    "patience = 50\n",
    "patience_count = 0\n",
    "\n",
    "previous_epoch_loss_valid = 300\n",
    "\n",
    "res = list()\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_loss, epoch_loss_valid, epoch_accuracy, epoch_accuracy_valid = [], [], [], []\n",
    "\n",
    "    np.random.shuffle(shuffle_train)\n",
    "    np.random.shuffle(shuffle_valid)\n",
    "\n",
    "\n",
    "    # Train\n",
    "    # batchSize=2\n",
    "    model.train()\n",
    "    for i in tqdm(range(0, len(ref_train), batchSize)):\n",
    "\n",
    "        incr_i = min(i+batchSize, len(ref_train))\n",
    "        indices = shuffle_train[i: incr_i]\n",
    "\n",
    "        targets_ = torch.tensor(y_train[indices])\n",
    "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
    "        \n",
    "        features_ = np.array(features_train)[indices]\n",
    "        features_ = np.vstack(features_)\n",
    "        features_ = torch.FloatTensor(features_).to(device)\n",
    "\n",
    "        cum_nodes = adj_shapes_train[indices]\n",
    "        idx_batch = np.repeat(np.arange(\n",
    "            batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
    "            ), cum_nodes)\n",
    "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "        \n",
    "        adj_ = adj_train[indices]\n",
    "        adj_ = sp.block_diag(adj_)\n",
    "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(features_, adj_, idx_batch, cum_nodes)\n",
    "        \n",
    "        del features_\n",
    "        del adj_\n",
    "        del idx_batch\n",
    "\n",
    "        # Backward\n",
    "        loss = criterion(output, targets_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        targets_ = targets_.to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "\n",
    "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy.append(accuracy)\n",
    "\n",
    "        del output\n",
    "\n",
    "    # Validation\n",
    "    batchSize=64\n",
    "    model.eval()\n",
    "    for i in range(0, len(ref_valid), int(batchSize)):\n",
    "\n",
    "        incr_i = min(i+int(batchSize), len(ref_valid))\n",
    "        indices = shuffle_valid[i: incr_i]\n",
    "\n",
    "        targets_ = torch.tensor(y_valid[indices])\n",
    "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
    "\n",
    "        features_ = np.array(features_train)[indices]\n",
    "        features_ = np.vstack(features_)\n",
    "        features_ = torch.FloatTensor(features_).to(device)\n",
    "\n",
    "        cum_nodes = adj_shapes_valid[indices]\n",
    "        idx_batch = np.repeat(np.arange(\n",
    "            int(batchSize) if incr_i%int(batchSize)==0 else incr_i%int(batchSize)\n",
    "            ), cum_nodes)\n",
    "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "        \n",
    "        adj_ = adj_valid[indices]\n",
    "        adj_ = sp.block_diag(adj_)\n",
    "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
    "\n",
    "        output = model(features_, adj_, idx_batch, cum_nodes)\n",
    "\n",
    "        del features_\n",
    "        del adj_\n",
    "        del idx_batch\n",
    "\n",
    "        # Compute metrics\n",
    "        loss = criterion(output, targets_)\n",
    "        epoch_loss_valid.append(loss.item())\n",
    "        \n",
    "        targets_ = targets_.to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "\n",
    "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy_valid.append(accuracy)\n",
    "\n",
    "        del output\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    epoch_loss = np.array(epoch_loss).mean()\n",
    "    epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
    "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
    "    epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
    "\n",
    "    tqdm.write(\n",
    "        '\\n'\n",
    "        f'Epoch {epoch}:\\ntrain loss: {round(epoch_loss, 4)}, '\n",
    "        f'valid loss: {round(epoch_loss_valid, 4)}, delta loss: {round(epoch_loss-epoch_loss_valid, 4)},\\nacc train: {round(epoch_accuracy, 4)}, '\n",
    "        f'acc valid: {round(epoch_accuracy_valid, 4)}'\n",
    "        )\n",
    "    \n",
    "    # Early stopping :\n",
    "    if previous_epoch_loss_valid < epoch_loss_valid:\n",
    "        if patience_count == 0:\n",
    "          torch.save({\"state\": model.state_dict(),}, \"last_model_checkpoint.pt\")\n",
    "        patience_count +=1\n",
    "        if patience_count == patience:\n",
    "          print(f'Early stopping end of epoch {epoch}')\n",
    "          break\n",
    "\n",
    "    else :\n",
    "        previous_epoch_loss_valid = epoch_loss_valid\n",
    "        patience_count = 0\n",
    "\n",
    "        # Save last best results\n",
    "        del res\n",
    "        res = list()\n",
    "        shuffle_test = np.arange(len(ref_test))\n",
    "\n",
    "        for i in range(0, len(ref_test), int(batchSize)):\n",
    "            model.eval()\n",
    "\n",
    "            incr_i = min(i+int(batchSize), len(ref_test))\n",
    "            indices = shuffle_test[i: incr_i]\n",
    "\n",
    "            features_ = np.array(features_test)[indices]\n",
    "            features_ = np.vstack(features_)\n",
    "            features_ = torch.FloatTensor(features_).to(device)\n",
    "\n",
    "            cum_nodes = adj_shapes_test[indices]\n",
    "            idx_batch = np.repeat(np.arange(\n",
    "                batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
    "                ), cum_nodes)\n",
    "            idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "            \n",
    "            adj_ = adj_test[indices]\n",
    "            adj_ = sp.block_diag(adj_)\n",
    "            adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
    "            \n",
    "            output = model(features_, adj_, idx_batch, cum_nodes)\n",
    "\n",
    "            res.append(output.to('cpu').detach().numpy())\n",
    "\n",
    "            del output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93201184ae5283544afdd58677953ee734bf67b299385e7b22daff49378f4f38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
