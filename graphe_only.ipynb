{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## COLAB AREA\n",
    "\n",
    "# !git clone https://github.com/AugustinCombes/zetaFold.git\n",
    "# %pip install transformers\n",
    "# # from google.colab import drive\n",
    "\n",
    "# # drive.mount('/content/gdrive/', force_remount=True)\n",
    "# # ../gdrive/MyDrive/data\n",
    "# %cd zetaFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import normalize_adjacency, sparse_mx_to_torch_sparse_tensor, load_data, submit_predictions, graph_train_valid_test\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# from models.enhanced_graph import GTN\n",
    "from models.self_attention_graph import GTN\n",
    "from models.baseline_graph import GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 4888\n",
    "treshold_valid = 733\n",
    "\n",
    "ref = dict()\n",
    "with open('data/graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) != 0:\n",
    "            if len([_ for _ in ref.values() if _ == \"train\"]) < n_labels - treshold_valid:\n",
    "              ref[i] = \"train\"\n",
    "            else :\n",
    "              ref[i] = \"valid\"\n",
    "        else:\n",
    "            ref[i] = \"test\"\n",
    "\n",
    "ref_train = np.array([i for i in range(len(ref)) if ref[i]==\"train\"])\n",
    "ref_valid = np.array([i for i in range(len(ref)) if ref[i]==\"valid\"])\n",
    "ref_test = np.array([i for i in range(len(ref)) if ref[i]==\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# device = 'mps'\n",
    "# device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "\n",
    "y = []\n",
    "valid_id = []\n",
    "\n",
    "with open('data/graph_labels.txt', \"r\") as f1:\n",
    "  for line in f1:\n",
    "    s1, s2 = line.strip().split(',')\n",
    "    if len(s2.strip())>0:\n",
    "      y.append(int(s2))\n",
    "    else :\n",
    "      valid_id.append(s1)\n",
    "\n",
    "y = np.array(y)\n",
    "y_train, y_valid = y[:-treshold_valid], y[-treshold_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph features\n",
    "\n",
    "adj, adj_weight, features = load_data()\n",
    "\n",
    "(\n",
    "    adj_train, adj_valid, adj_test, \n",
    "    features_train, features_valid, features_test, \n",
    "    adj_shapes_train, adj_shapes_valid, adj_shapes_test\n",
    "                                                        ) = graph_train_valid_test(adj, adj_weight, features, ref_train, ref_valid, ref_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essai avec GTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTN(90, 64, 0.2, 18).to(device)\n",
    "model = GNN(90, 64, 0.2, 18).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_data(features, adj_shapes, adj, indices, batchSize, incr_i, device):\n",
    "    features_ = np.array(features)[indices]\n",
    "    features_ = np.vstack(features_)\n",
    "    features_ = torch.FloatTensor(features_).to(device)\n",
    "\n",
    "    cum_nodes = adj_shapes[indices]\n",
    "    idx_batch = np.repeat(np.arange(\n",
    "        batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
    "        ), cum_nodes)\n",
    "    idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "\n",
    "    adj_ = adj[indices]\n",
    "    adj_ = sp.block_diag(adj_)\n",
    "    adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
    "\n",
    "    return features_, adj_, idx_batch, cum_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = np.array([440.,  50., 939.,  60., 112., 625., 202.,  74., 998.,  57.,  43.,305.,  44.,  59., 548., 226.,  60.,  46.])\n",
    "weights = 1/pops\n",
    "weights = weights/np.mean(weights)\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to('cuda')\n",
    "balanced_criterion = nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1,\n",
    "    # lr=1e-3,\n",
    "    weight_decay=1e-5\n",
    "    )\n",
    "epochs = 70\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "                lr_lambda=lambda epoch: 1e-4 + (1e-4 - 5e-3) * ((epoch - epochs)/ epochs))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "batchSize = 64 #training batchsize\n",
    "\n",
    "patience = 50\n",
    "patience_count = 0\n",
    "\n",
    "best_valid_loss = 300\n",
    "previous_epoch_loss_valid = 300\n",
    "\n",
    "res = list()\n",
    "shuffle_train = np.arange(len(ref_train))\n",
    "shuffle_valid = np.arange(len(ref_valid))\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_loss, epoch_loss_valid, epoch_accuracy, epoch_accuracy_valid = [], [], [], []\n",
    "    epoch_nll, epoch_nll_valid = [], []\n",
    "\n",
    "    np.random.shuffle(shuffle_train)\n",
    "    np.random.shuffle(shuffle_valid)\n",
    "\n",
    "    if epoch == 50 :\n",
    "        criterion = balanced_criterion\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for i in range(0, len(ref_train), batchSize):\n",
    "        incr_i = min(i+batchSize, len(ref_train))\n",
    "        indices = shuffle_train[i: incr_i]\n",
    "\n",
    "        targets_ = torch.tensor(y_train[indices])\n",
    "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
    "\n",
    "        features_, adj_, idx_batch, cum_nodes = process_batch_data(features_train, adj_shapes_train, adj_train, indices, batchSize, incr_i, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features_, adj_, idx_batch)#, cum_nodes)\n",
    "\n",
    "        # Backward\n",
    "        loss = criterion(output, targets_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics\n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_nll.append(nn.CrossEntropyLoss()(output, targets_).item())\n",
    "        \n",
    "        targets_ = targets_.to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "\n",
    "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy.append(accuracy)\n",
    "\n",
    "    # Validation\n",
    "    batchSizeVal=64\n",
    "    model.eval()\n",
    "    for i in range(0, len(ref_valid), int(batchSizeVal)):\n",
    "        incr_i = min(i+int(batchSizeVal), len(ref_valid))\n",
    "        indices = shuffle_valid[i: incr_i]\n",
    "\n",
    "        targets_ = torch.tensor(y_valid[indices])\n",
    "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
    "\n",
    "        features_, adj_, idx_batch, cum_nodes = process_batch_data(features_valid, adj_shapes_valid, adj_valid, indices, batchSizeVal, incr_i, device)\n",
    "\n",
    "        output = model(features_, adj_, idx_batch)#, cum_nodes)\n",
    "\n",
    "        # Compute metrics\n",
    "        loss = criterion(output, targets_)\n",
    "        epoch_loss_valid.append(loss.item())\n",
    "        epoch_nll_valid.append(nn.CrossEntropyLoss()(output, targets_).item())\n",
    "        \n",
    "        targets_ = targets_.to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "\n",
    "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy_valid.append(accuracy)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss = np.array(epoch_loss).mean()\n",
    "    epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
    "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
    "    epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
    "\n",
    "    tqdm.write(\n",
    "        '\\n'\n",
    "        f'Epoch {epoch}:\\ntrain loss: {round(epoch_loss, 4)}, '\n",
    "        f'valid loss: {round(epoch_loss_valid, 4)}, delta loss: {round(epoch_loss-epoch_loss_valid, 4)},\\nacc train: {round(epoch_accuracy, 4)}, '\n",
    "        f'acc valid: {round(epoch_accuracy_valid, 4)}'\n",
    "        )\n",
    "    \n",
    "    # Early stopping :\n",
    "    if previous_epoch_loss_valid < epoch_loss_valid:\n",
    "        patience_count +=1\n",
    "        if patience_count == patience:\n",
    "          print(f'Early stopping end of epoch {epoch}')\n",
    "          break\n",
    "\n",
    "    else :\n",
    "        previous_epoch_loss_valid = epoch_loss_valid\n",
    "        patience_count = 0\n",
    "\n",
    "        # Save last best results\n",
    "        res = list()\n",
    "        shuffle_test = np.arange(len(ref_test))\n",
    "\n",
    "        for i in range(0, len(ref_test), int(batchSizeVal)):\n",
    "            model.eval()\n",
    "\n",
    "            incr_i = min(i+int(batchSizeVal), len(ref_test))\n",
    "            indices = shuffle_test[i: incr_i]\n",
    "\n",
    "            features_, adj_, idx_batch, cum_nodes = process_batch_data(features_test, adj_shapes_test, adj_test, indices, batchSizeVal, incr_i, device)\n",
    "            \n",
    "            output = model(features_, adj_, idx_batch)#, cum_nodes)\n",
    "\n",
    "            res.append(output.to('cpu').detach().numpy())\n",
    "\n",
    "    if epoch_loss_valid < best_valid_loss :\n",
    "        torch.save({\"state\": model.state_dict(),}, \"last_model_checkpoint.pt\")\n",
    "        best_valid_loss = epoch_loss_valid \n",
    "        print('oui')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best valid loss so far : 1.68\\\n",
    "1.7445 en submission\\\n",
    "with lr=1e-3,\n",
    "    weight_decay=1e-5\n",
    "epochs = 70\n",
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "sub = list(map(lambda x: np.array(nn.Softmax(dim=1)(torch.tensor(x))), res))\n",
    "\n",
    "y_pred = np.concatenate(sub, axis=0)\n",
    "y_pred.shape\n",
    "\n",
    "proteins_test = list()\n",
    "with open('data/graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            proteins_test.append(t[0])\n",
    "\n",
    "with open('graphedemerde70.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    lst = list()\n",
    "    for i in range(18):\n",
    "        lst.append('class'+str(i))\n",
    "    lst.insert(0, \"name\")\n",
    "    writer.writerow(lst)\n",
    "    for i, protein in enumerate(proteins_test):\n",
    "        lst = y_pred[i,:].tolist()\n",
    "        lst.insert(0, protein)\n",
    "        writer.writerow(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93201184ae5283544afdd58677953ee734bf67b299385e7b22daff49378f4f38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
