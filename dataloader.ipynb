{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import normalize_adjacency, sparse_mx_to_torch_sparse_tensor, load_data, submit_predictions\n",
    "from dataloader import get_loader\n",
    "\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj, weights, features, edge_features, new = load_data()\n",
    "\n",
    "# adj = [normalize_adjacency(adj[i], weights[i]) for i in range(len(adj))]\n",
    "\n",
    "ref = list()\n",
    "with open('data/graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) == 0:\n",
    "            ref.append(True)\n",
    "        else:\n",
    "            ref.append(False)\n",
    "\n",
    "ref = np.array(ref)\n",
    "ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# device = 'mps'\n",
    "# device = 'cuda'\n",
    "\n",
    "version = \"train\"\n",
    "# version = \"valid\"\n",
    "n_labels = 4888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_kept = []\n",
    "with open('data/graph_labels.txt', \"r\") as f1:\n",
    "    for line in f1:\n",
    "        s1, s2 = line.strip().split(',')\n",
    "        if len(s2.strip())>0:\n",
    "            is_kept.append(version != \"valid\")\n",
    "        else :\n",
    "            is_kept.append(version == \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLAB AREA\n",
    "\n",
    "# !git clone https://github.com/AugustinCombes/zetaFold.git\n",
    "# %pip install transformers\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/gdrive/', force_remount=True)\n",
    "# #../content/gdrive/MyDrive/dataltegrad\n",
    "# %cd zetaFold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENCES ONLY : Finetune DistillProtbert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'yarongef/DistilProtBert'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define probert-based classifier\n",
    " \n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ProteinClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME).to(device)\n",
    "        self.bert.eval()\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes).to(device)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.classifier(nn.ReLU()(output.last_hidden_state[:, 0, :]))\n",
    "        return nn.LogSoftmax(dim=1)(output)\n",
    "\n",
    "model = ProteinClassifier(18).to(device)\n",
    "\n",
    "for module in model.bert.encoder.layer[0:-1]:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "    lr = 1,\n",
    "    #weight_decay=0.01\n",
    ")\n",
    "epochs = 10\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "                lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-3) * ((epoch - epochs)/ epochs))\n",
    "\n",
    "data_loader = get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
    "                tokenizer=tokenizer, max_len=600, batch_size=8, shuffle=True, version=version, drop_last=True)\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_loss, epoch_log_loss, epoch_accuracy = [], [], []\n",
    "    \n",
    "    for batch_num, e in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # Compute forward pass\n",
    "        input = e['input_ids'].to(device)\n",
    "        src_mask = e['attention_mask'].to(device)\n",
    "        output = model(input, src_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        target = e['target'].reshape(-1).to(device)\n",
    "        output = output.to(device)\n",
    "        loss = criterion(output, target).to(device)\n",
    "\n",
    "        # print('target', target, 'prediction', torch.argmax(output, dim=-1))\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        target = torch.nn.functional.one_hot(target, 18).to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "        nll_loss = log_loss(target, output)\n",
    "        epoch_log_loss.append(nll_loss)\n",
    "\n",
    "        accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy.append(accuracy)\n",
    "\n",
    "        # if batch_num > 2:\n",
    "        #     break\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss = np.array(epoch_loss).mean()\n",
    "    epoch_log_loss = np.array(epoch_log_loss).mean()\n",
    "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
    "\n",
    "    pbar.set_description(\n",
    "        f'Epoch {epoch}, cel: {round(epoch_loss, 4)}, '\n",
    "        f'nll: {round(epoch_log_loss, 4)}, acc: {round(epoch_accuracy, 4)}'\n",
    "        )\n",
    "    # break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPHE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj, adj_weight, features, edge_features, new = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj = [normalize_adjacency(A, W) for A, W in zip(adj, adj_weight)]\n",
    "# adj_shapes = np.array([at.shape[0] for at in adj])\n",
    "# adj = [adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare graph features\n",
    "\n",
    "# 1. Full graph-related database in arrays\n",
    "\n",
    "adj, adj_weight, features, edge_features, Flist = load_data()\n",
    "del Flist\n",
    "del edge_features\n",
    "\n",
    "adj = [normalize_adjacency(A, W) for A, W in zip(adj, adj_weight)]\n",
    "adj_shapes = np.array([at.shape[0] for at in adj])\n",
    "adj = [adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))]\n",
    "\n",
    "adj = np.array(adj)[np.array(is_kept)]\n",
    "features = np.array(features, dtype=object)[np.array(is_kept)]\n",
    "adj_shapes = adj_shapes[np.array(is_kept)]\n",
    "\n",
    "# features = features[:n_labels] if version != \"valid\" else features[n_labels:]\n",
    "# adj = adj[:n_labels] if version != \"valid\" else adj[n_labels:]\n",
    "\n",
    "## 2. Then, we load batchs thanks to \"indexs\" key of dataloader element\n",
    "\n",
    "## Example : loading only the first batch of graph-related data\n",
    "# for e in data_loader:\n",
    "#     break\n",
    "# batch_indices = e['indexs'] ## Indexs that are part of the batch\n",
    "\n",
    "# features_ = np.array(features)[batch_indices]\n",
    "# features_ = np.vstack(features_)\n",
    "# features_ = torch.FloatTensor(features_).to(device)\n",
    "\n",
    "# adj_ = adj[batch_indices]\n",
    "# adj_ = sp.block_diag(adj_)\n",
    "# adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
    "\n",
    "# cum_nodes = adj_shapes[batch_indices]\n",
    "# idx_batch = np.repeat(np.arange(\n",
    "#     len(batch_indices)\n",
    "#     ), cum_nodes)\n",
    "# idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "\n",
    "# # With GNN model\n",
    "\n",
    "# model = GNN(...).to(device)\n",
    "# output = model(features_, adj_, idx_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple message passing model that consists of 2 message passing layers\n",
    "    and the sum aggregation function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, dropout, n_class):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # self.fc2 = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, n_class)\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_in, adj, idx):\n",
    "        # first message passing layer\n",
    "        x = self.fc1(x_in)\n",
    "        x = self.relu(torch.mm(adj, x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # second message passing layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(torch.mm(adj, x))\n",
    "        \n",
    "        # sum aggregator\n",
    "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
    "        out = torch.zeros(torch.max(idx)+1, x.size(1)).to(x_in.device)\n",
    "        out = out.scatter_add_(0, idx, x)\n",
    "        \n",
    "        # batch normalization layer\n",
    "        out = self.bn(out)\n",
    "\n",
    "        # mlp to produce output\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(86, 64, 0.1, 18).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "#                 lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-3) * ((epoch - epochs)/ epochs))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "data_loader = get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
    "                tokenizer=tokenizer, max_len=600, batch_size=64, shuffle=False, version=version, drop_last=True)\n",
    "                \n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_loss, epoch_log_loss, epoch_accuracy = [], [], []\n",
    "\n",
    "    for batch_num, e in enumerate(data_loader):\n",
    "        \n",
    "        batch_indices = e['indexs']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        target = F.one_hot(e['target'], 18).float().to(device)\n",
    "\n",
    "        # Compute graph forward\n",
    "        features_ = np.array(features)[batch_indices]\n",
    "        features_ = np.vstack(features_)\n",
    "        features_ = torch.FloatTensor(features_).to(device)\n",
    "\n",
    "        adj_ = adj[batch_indices]\n",
    "        adj_ = sp.block_diag(adj_)\n",
    "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
    "\n",
    "        cum_nodes = adj_shapes[batch_indices]\n",
    "        idx_batch = np.repeat(np.arange(\n",
    "            len(batch_indices)\n",
    "            ), cum_nodes)\n",
    "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "        \n",
    "        output = model(features_, adj_, idx_batch)\n",
    "\n",
    "        # Backward\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        target = target.to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "        nll_loss = log_loss(target, output)\n",
    "        epoch_log_loss.append(nll_loss)\n",
    "\n",
    "        accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy.append(accuracy)\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    epoch_loss = np.array(epoch_loss).mean()\n",
    "    epoch_log_loss = np.array(epoch_log_loss).mean()\n",
    "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
    "\n",
    "    pbar.set_description(\n",
    "        f'Epoch {epoch}, cel: {round(epoch_loss, 4)}, '\n",
    "        f'nll: {round(epoch_log_loss, 4)}, acc: {round(epoch_accuracy, 4)}'\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTICULTURAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatModel(nn.Module):\n",
    "    def __init__(self, out_dim, feature_dim, graph_dim):\n",
    "        super(ConcatModel, self).__init__()\n",
    "        self.seqModel = ProteinClassifier(out_dim).to(device)\n",
    "        self.graphModel = GNN(feature_dim, graph_dim, dropout=0.25, n_class=out_dim).to(device)\n",
    "\n",
    "        self.classifier = nn.Linear(2*out_dim, 18)\n",
    "        self.activation = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x_in, adj, idx, input_ids, attention_mask):\n",
    "        seqEmbedding = self.seqModel(input_ids, attention_mask)\n",
    "        graphEmbedding = self.graphModel(x_in, adj, idx)\n",
    "        \n",
    "        out = torch.concat([graphEmbedding, seqEmbedding], dim=1)\n",
    "        out = nn.Dropout(0.2)(out)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return self.activation(out)\n",
    "\n",
    "model = ConcatModel(64, 86, 128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in model.seqModel.bert.encoder.layer[0:-1]:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "epochs = 500\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "                lr_lambda=lambda epoch: 1e-6 + (1e-6 - 1e-3) * ((epoch - epochs)/ epochs))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "data_loader = get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
    "                tokenizer=tokenizer, max_len=600, batch_size=8, shuffle=False, version=version, drop_last=True)\n",
    "                \n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_loss, epoch_log_loss, epoch_accuracy = [], [], []\n",
    "\n",
    "    for batch_num, e in enumerate(data_loader):\n",
    "        \n",
    "        batch_indices = e['indexs']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        target = F.one_hot(e['target'], 18).float().to(device)\n",
    "\n",
    "        #Compute sequence forward\n",
    "        input = e['input_ids'].to(device)\n",
    "        src_mask = e['attention_mask'].to(device)\n",
    "\n",
    "        # Compute graph forward\n",
    "        features_ = np.array(features)[batch_indices]\n",
    "        features_ = np.vstack(features_)\n",
    "        features_ = torch.FloatTensor(features_).to(device)\n",
    "\n",
    "        adj_ = adj[batch_indices]\n",
    "        adj_ = sp.block_diag(adj_)\n",
    "        adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
    "\n",
    "        cum_nodes = adj_shapes[batch_indices]\n",
    "        idx_batch = np.repeat(np.arange(\n",
    "            len(batch_indices)\n",
    "            ), cum_nodes)\n",
    "        idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "        \n",
    "        output = model(features_, adj_, idx_batch, input, src_mask)\n",
    "\n",
    "        # Backward\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics\n",
    "        epoch_loss.append(loss.item())\n",
    "        print(epoch_loss[-1])\n",
    "        \n",
    "        target = target.to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "        nll_loss = log_loss(target, output)\n",
    "        epoch_log_loss.append(nll_loss)\n",
    "\n",
    "        accuracy = (target.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy.append(accuracy)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss = np.array(epoch_loss).mean()\n",
    "    epoch_log_loss = np.array(epoch_log_loss).mean()\n",
    "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
    "\n",
    "    pbar.set_description(\n",
    "        f'Epoch {epoch}, cel: {round(epoch_loss, 4)}, '\n",
    "        f'nll: {round(epoch_log_loss, 4)}, acc: {round(epoch_accuracy, 4)}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTN version brouillon\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "a = torch.ones(25, 300)\n",
    "b = torch.ones(22, 300)\n",
    "c = torch.ones(15, 300)\n",
    "pad_sequence([a, b, c]).size()\n",
    "\n",
    "\n",
    "\n",
    "from graph_transformer_pytorch import GraphTransformer\n",
    "\n",
    "model = GraphTransformer(\n",
    "    dim = 256,\n",
    "    depth = 6,\n",
    "    edge_dim = 512,             # optional - if left out, edge dimensions is assumed to be the same as the node dimensions above\n",
    "    with_feedforwards = True,   # whether to add a feedforward after each attention layer, suggested by literature to be needed\n",
    "    gated_residual = True,      # to use the gated residual to prevent over-smoothing\n",
    "    rel_pos_emb = True          # set to True if the nodes are ordered, default to False\n",
    ")\n",
    "\n",
    "nodes = torch.randn(1, 128, 256)\n",
    "edges = torch.randn(1, 128, 128, 512)\n",
    "mask = torch.ones(1, 128).bool()\n",
    "\n",
    "nodes, edges = model(nodes, edges, mask = mask)\n",
    "\n",
    "nodes.shape # (1, 128, 256) - project to R^3 for coordinates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93201184ae5283544afdd58677953ee734bf67b299385e7b22daff49378f4f38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
