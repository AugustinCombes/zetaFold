{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils.utils import normalize_adjacency, sparse_mx_to_torch_sparse_tensor, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"train\"\n",
    "version = \"valid\"\n",
    "n_labels = 4888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/c3y45k5d5gdbmk7pnky9y19c0000gn/T/ipykernel_3665/1356191593.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  features = np.array(features)\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "adj, features, edge_features = load_data()\n",
    "del edge_features\n",
    "adj = [normalize_adjacency(A) for A in adj]\n",
    "adj_shapes = np.array([at.shape[0] for at in adj])\n",
    "adj = [adj[idx] + sp.identity(adj_shapes[idx]) for idx in range(len(adj))]\n",
    "\n",
    "adj = np.array(adj)\n",
    "features = np.array(features)\n",
    "\n",
    "features = features[:n_labels] if version != \"valid\" else features[n_labels:]\n",
    "adj = adj[:n_labels] if version != \"valid\" else adj[n_labels:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1223, 1223)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features), len(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [] \n",
    "# with open('data/sequences.txt', \"r\") as f1:\n",
    "#     for line in f1:\n",
    "#         documents.append(' '.join(list(line[:-1])))\n",
    "\n",
    "# documents = documents[:n_labels:] if version != \"valid\" else documents[:n_labels] # String protein sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, path_documents, path_labels, tokenizer, max_len, version):\n",
    "        self.version = version # \"train\" (features & labels) / \"test\" (features & labels) / \"valid\" (features)\n",
    "        self.n_labels = 4888 if version != \"valid\" else 1223\n",
    "        ## TODO valid\n",
    "\n",
    "        # Labels\n",
    "        self.labels = [] # Class labels\n",
    "        self.valid_id = []\n",
    "        with open(path_labels, \"r\") as f1:\n",
    "            for line in f1:\n",
    "                s1, s2 = line.strip().split(',')\n",
    "                if len(s2.strip())>0:\n",
    "                    self.labels.append(int(s2))\n",
    "                else :\n",
    "                    self.valid_id.append(s1)\n",
    "\n",
    "\n",
    "        # Protein sequences\n",
    "        self.max_len = max_len # Maximum sequence length threshold, max in train database is 989\n",
    "        self.tokenizer = tokenizer # Language model tokenizer\n",
    "        \n",
    "        documents = [] \n",
    "        with open(path_documents, \"r\") as f1:\n",
    "            for line in f1:\n",
    "                documents.append(' '.join(list(line[:-1])))\n",
    "                \n",
    "        self.documents = documents[:self.n_labels] if version != \"valid\" else documents[-self.n_labels:] # String protein sequences\n",
    "\n",
    "        if version=='valid':\n",
    "            self.labels = [-1]*self.n_labels #dummy labels for valid\n",
    "\n",
    "        print(self.n_labels, len(self.labels),len(self.documents))\n",
    "\n",
    "        assert len(self.labels) == len(self.documents)\n",
    "\n",
    "        # Graphe features\n",
    "        # Inplace with getitem's indexs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "\n",
    "        # Load protein sequence\n",
    "        sequence = self.documents[index].split()\n",
    "        if len(sequence) > self.max_len - 1:\n",
    "            sequence = sequence[:self.max_len-1]\n",
    "            \n",
    "        # Tokenize the sequence\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Label\n",
    "        target = [self.labels[index]]\n",
    "\n",
    "        sample = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'indexs': index,\n",
    "            \"target\": torch.tensor(target),\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "def get_loader(path_documents='data/sequences.txt', path_labels='data/graph_labels.txt', \n",
    "                tokenizer=None, max_len=600, batch_size=16, shuffle=False, version=version):\n",
    "        \n",
    "    dataset = Dataset(path_documents=path_documents, path_labels=path_labels, tokenizer=tokenizer, max_len=600, version=version)\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=True, drop_last=False)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223 1223 1223\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'yarongef/DistilProtBert'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=False)\n",
    "\n",
    "loader = get_loader(tokenizer=tokenizer, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2, 12, 17,  ...,  0,  0,  0],\n",
       "         [ 2, 11, 17,  ...,  0,  0,  0],\n",
       "         [ 2, 10, 13,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 2, 10, 24,  ...,  0,  0,  0],\n",
       "         [ 2, 23,  6,  ...,  0,  0,  0],\n",
       "         [ 2, 20, 10,  ...,  0,  0,  0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'indexs': tensor([4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887]),\n",
       " 'target': tensor([[11],\n",
       "         [ 0],\n",
       "         [14],\n",
       "         [14],\n",
       "         [14],\n",
       "         [ 5],\n",
       "         [14],\n",
       "         [14]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for e in loader:\n",
    "    ()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)[e['indexs']]\n",
    "features = np.vstack(features)\n",
    "features = torch.FloatTensor(features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj[e['indexs']]\n",
    "adj = sp.block_diag(adj)\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93201184ae5283544afdd58677953ee734bf67b299385e7b22daff49378f4f38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
