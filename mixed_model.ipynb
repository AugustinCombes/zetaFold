{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## COLAB AREA\n",
    "\n",
    "# !git clone https://github.com/AugustinCombes/zetaFold.git\n",
    "# %pip install transformers\n",
    "# # from google.colab import drive\n",
    "\n",
    "# # drive.mount('/content/gdrive/', force_remount=True)\n",
    "# # ../gdrive/MyDrive/data\n",
    "# %cd zetaFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import normalize_adjacency, sparse_mx_to_torch_sparse_tensor, load_data, submit_predictions, graph_train_valid_test\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# from models.enhanced_graph import GTN\n",
    "from models.self_attention_graph import GTN\n",
    "from models.baseline_graph import GNN\n",
    "from models.sequence_protbert import ProteinClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 4888\n",
    "treshold_valid = 733\n",
    "\n",
    "ref = dict()\n",
    "with open('data/graph_labels.txt', 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        t = line.split(',')\n",
    "        if len(t[1][:-1]) != 0:\n",
    "            if len([_ for _ in ref.values() if _ == \"train\"]) < n_labels - treshold_valid:\n",
    "              ref[i] = \"train\"\n",
    "            else :\n",
    "              ref[i] = \"valid\"\n",
    "        else:\n",
    "            ref[i] = \"test\"\n",
    "\n",
    "ref_train = np.array([i for i in range(len(ref)) if ref[i]==\"train\"])\n",
    "ref_valid = np.array([i for i in range(len(ref)) if ref[i]==\"valid\"])\n",
    "ref_test = np.array([i for i in range(len(ref)) if ref[i]==\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# device = 'mps'\n",
    "# device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "\n",
    "y = []\n",
    "valid_id = []\n",
    "\n",
    "with open('data/graph_labels.txt', \"r\") as f1:\n",
    "  for line in f1:\n",
    "    s1, s2 = line.strip().split(',')\n",
    "    if len(s2.strip())>0:\n",
    "      y.append(int(s2))\n",
    "    else :\n",
    "      valid_id.append(s1)\n",
    "\n",
    "y = np.array(y)\n",
    "y_train, y_valid = y[:-treshold_valid], y[-treshold_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph features\n",
    "\n",
    "adj, adj_weight, features = load_data()\n",
    "\n",
    "(\n",
    "    adj_train, adj_valid, adj_test, \n",
    "    features_train, features_valid, features_test, \n",
    "    adj_shapes_train, adj_shapes_valid, adj_shapes_test\n",
    "                                                        ) = graph_train_valid_test(adj, adj_weight, features, ref_train, ref_valid, ref_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences\n",
    "\n",
    "sequences = [] \n",
    "with open('data/sequences.txt', \"r\") as f1:\n",
    "  for line in f1:\n",
    "    sequences.append(' '.join(list(line[:-1])))\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "sequences_train, sequences_valid, sequences_test = sequences[ref_train], sequences[ref_valid], sequences[ref_test]\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('yarongef/DistilProtBert', do_lower_case=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.sequence_protbert import ProteinClassifier\n",
    "from transformers import BertModel\n",
    "\n",
    "class MixedModel(nn.Module):\n",
    "    def __init__(self, dropout=0.8, device=\"cuda\"):\n",
    "        super(MixedModel, self).__init__()\n",
    "      \n",
    "        sequences_model = ProteinClassifier(18).to(device)\n",
    "        #load weights\n",
    "        self.bert = sequences_model.bert\n",
    "        \n",
    "        self.graph_model = GNN(90, 64, 0.2, 18).to(device)\n",
    "        #load weights\n",
    "        \n",
    "        self.mlp = nn.Linear(self.bert.config.hidden_size + 64, 18).to(device)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x_in, adj, idx, input_ids, attention_mask):\n",
    "\n",
    "        # first message passing layer\n",
    "        x = self.graph_model.fc1(x_in)\n",
    "        x = self.graph_model.relu(torch.mm(adj, x))\n",
    "\n",
    "        # second message passing layer\n",
    "        x = self.graph_model.fc2(x)\n",
    "        x = self.graph_model.relu(torch.mm(adj, x))\n",
    "        \n",
    "        # sum aggregator\n",
    "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
    "        out = torch.zeros(torch.max(idx)+1, x.size(1)).to(x_in.device)\n",
    "        out = out.scatter_add_(0, idx, x)\n",
    "        \n",
    "        # batch normalization layer\n",
    "        out = self.graph_model.bn(out)\n",
    "\n",
    "        # mlp to produce embedding\n",
    "        out = self.graph_model.relu(self.graph_model.fc3(out))\n",
    "\n",
    "        output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        output = output.last_hidden_state[:, 0, :] #get cls token embedding\n",
    "        output = nn.Dropout(self.dropout)(output)\n",
    "\n",
    "        joined = torch.cat([output, out], dim=1)\n",
    "        joined = self.mlp(joined)\n",
    "        \n",
    "        return F.log_softmax(joined, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixedModel(device=device)\n",
    "\n",
    "for module in model.bert.encoder.layer[:]:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = np.array([440.,  50., 939.,  60., 112., 625., 202.,  74., 998.,  57.,  43.,305.,  44.,  59., 548., 226.,  60.,  46.])\n",
    "weights = 1/pops\n",
    "weights = weights/np.mean(weights)\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "balanced_criterion = nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s : tokenizer.encode_plus(\n",
    "            s,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=989,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_data(features, adj_shapes, adj, sequences, indices, batchSize, incr_i, device):\n",
    "    features_ = np.array(features)[indices]\n",
    "    features_ = np.vstack(features_)\n",
    "    features_ = torch.FloatTensor(features_).to(device)\n",
    "\n",
    "    cum_nodes = adj_shapes[indices]\n",
    "    idx_batch = np.repeat(np.arange(\n",
    "        batchSize if incr_i%batchSize==0 else incr_i%batchSize\n",
    "        ), cum_nodes)\n",
    "    idx_batch = torch.LongTensor(idx_batch).to(device)\n",
    "\n",
    "    adj_ = adj[indices]\n",
    "    adj_ = sp.block_diag(adj_)\n",
    "    adj_ = sparse_mx_to_torch_sparse_tensor(adj_).to(device)\n",
    "\n",
    "    sequences_ = torch.concat([encode(s)['input_ids'] for s in sequences[indices]]).to(device)\n",
    "\n",
    "    attention_mask_ = torch.concat([encode(s)['attention_mask'] for s in sequences[indices]]).to(device)\n",
    "\n",
    "    return features_, adj_, idx_batch, cum_nodes, sequences_, attention_mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=1,\n",
    "    # lr=1e-3,\n",
    "    # weight_decay=1e-3\n",
    "    )\n",
    "epochs = 20\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, \n",
    "                lr_lambda=lambda epoch: 1e-3 + (1e-3 - 1e-3) * ((epoch - epochs)/ epochs))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = balanced_criterion\n",
    "\n",
    "batchSize = 2 #training batchsize\n",
    "\n",
    "patience = 10\n",
    "patience_count = 0\n",
    "\n",
    "best_valid_loss = 300\n",
    "previous_epoch_loss_valid = 300\n",
    "\n",
    "res = list()\n",
    "shuffle_train = np.arange(len(ref_train))\n",
    "shuffle_valid = np.arange(len(ref_valid))\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    epoch_loss, epoch_loss_valid, epoch_accuracy, epoch_accuracy_valid = [], [], [], []\n",
    "    epoch_nll_train, epoch_nll_valid = [], []\n",
    "\n",
    "    np.random.shuffle(shuffle_train)\n",
    "    np.random.shuffle(shuffle_valid)\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for i in range(0, len(ref_train), batchSize):\n",
    "        incr_i = min(i+batchSize, len(ref_train))\n",
    "        indices = shuffle_train[i: incr_i]\n",
    "\n",
    "        targets_ = torch.tensor(y_train[indices])\n",
    "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
    "\n",
    "        features_, adj_, idx_batch, cum_nodes, sequences_, attention_mask_ = process_batch_data(\n",
    "            features_train, adj_shapes_train, adj_train, sequences_train, indices, batchSize, incr_i, device\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features_, adj_, idx_batch, sequences_, attention_mask_)#, cum_nodes)\n",
    "\n",
    "        # Backward\n",
    "        loss = criterion(output, targets_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics\n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_nll_train.append(nn.CrossEntropyLoss()(output, targets_).item())\n",
    "        \n",
    "        targets_ = targets_.to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "\n",
    "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy.append(accuracy)\n",
    "\n",
    "    # Validation\n",
    "    batchSizeVal=64\n",
    "    model.eval()\n",
    "    for i in range(0, len(ref_valid), int(batchSizeVal)):\n",
    "        incr_i = min(i+int(batchSizeVal), len(ref_valid))\n",
    "        indices = shuffle_valid[i: incr_i]\n",
    "\n",
    "        targets_ = torch.tensor(y_valid[indices])\n",
    "        targets_ = F.one_hot(targets_, 18).float().to(device)\n",
    "\n",
    "        features_, adj_, idx_batch, cum_nodes, sequences_, attention_mask_ = process_batch_data(\n",
    "            features_valid, adj_shapes_valid, adj_valid, sequences_valid, indices, batchSizeVal, incr_i, device\n",
    "            )\n",
    "\n",
    "        output = model(features_, adj_, idx_batch, sequences_, attention_mask_)#, cum_nodes)\n",
    "\n",
    "        # Compute metrics\n",
    "        loss = criterion(output, targets_)\n",
    "        epoch_loss_valid.append(loss.item())\n",
    "        epoch_nll_valid.append(nn.CrossEntropyLoss()(output, targets_).item())\n",
    "        \n",
    "        targets_ = targets_.to('cpu').detach().numpy()\n",
    "        output = nn.Softmax(dim=1)(output).to('cpu').detach().numpy()\n",
    "\n",
    "        accuracy = (targets_.argmax(axis=1)==output.argmax(axis=1)).mean()\n",
    "        epoch_accuracy_valid.append(accuracy)\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    epoch_loss = np.array(epoch_loss).mean()\n",
    "    epoch_loss_valid = np.array(epoch_loss_valid).mean()\n",
    "    epoch_accuracy = np.array(epoch_accuracy).mean()\n",
    "    epoch_accuracy_valid = np.array(epoch_accuracy_valid).mean()\n",
    "    epoch_nll_train = np.array(epoch_nll_train).mean()\n",
    "    epoch_nll_valid = np.array(epoch_nll_valid).mean()\n",
    "\n",
    "    if epoch_nll_valid < best_valid_loss :\n",
    "        torch.save({\"state\": model.state_dict(),}, \"last_model_checkpoint.pt\")\n",
    "        best_valid_loss = epoch_loss_valid \n",
    "\n",
    "    # tqdm.write(\n",
    "    pbar.set_description(\n",
    "        '\\n'\n",
    "        f'Epoch {epoch}:\\ntrain loss: {round(epoch_loss, 4)}, '\n",
    "        f'valid loss: {round(epoch_loss_valid, 4)}, delta loss: {round(epoch_loss-epoch_loss_valid, 4)},\\nacc train: {round(epoch_accuracy, 4)}, '\n",
    "        f'acc valid: {round(epoch_accuracy_valid, 4)}\\n'\n",
    "        f\"nll train: {round(epoch_nll_train, 4)}, nll valid: {round(epoch_nll_valid, 4)}\\n\"\n",
    "        f\"best valid: {round(best_valid_loss, 4)}\"\n",
    "        )\n",
    "    \n",
    "    # Early stopping :\n",
    "    if previous_epoch_loss_valid < epoch_loss_valid:\n",
    "        patience_count +=1\n",
    "        if patience_count == patience:\n",
    "          print(f'Early stopping end of epoch {epoch}')\n",
    "          break\n",
    "\n",
    "    else :\n",
    "        previous_epoch_loss_valid = epoch_loss_valid\n",
    "        patience_count = 0\n",
    "\n",
    "        # Save last best results\n",
    "        res = list()\n",
    "        shuffle_test = np.arange(len(ref_test))\n",
    "\n",
    "        for i in range(0, len(ref_test), int(batchSizeVal)):\n",
    "            model.eval()\n",
    "\n",
    "            incr_i = min(i+int(batchSizeVal), len(ref_test))\n",
    "            indices = shuffle_test[i: incr_i]\n",
    "\n",
    "            features_, adj_, idx_batch, cum_nodes, sequences_, attention_mask_ = process_batch_data(\n",
    "                features_test, adj_shapes_test, adj_test, sequences_test, indices, batchSizeVal, incr_i, device\n",
    "                )\n",
    "            \n",
    "            output = model(features_, adj_, idx_batch, sequences_, attention_mask_)#, cum_nodes)\n",
    "\n",
    "            res.append(output.to('cpu').detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altegrad_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93201184ae5283544afdd58677953ee734bf67b299385e7b22daff49378f4f38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
